{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You have a CUDA device, so you should probably run with --cuda\n"
     ]
    }
   ],
   "source": [
    "# implemented using https://github.com/pytorch/examples/tree/master/word_language_model\n",
    "\n",
    "import torch\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import time\n",
    "import math\n",
    "import nltk\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "torch.manual_seed(1111)\n",
    "if torch.cuda.is_available():\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#logging\n",
    "!touch app.log\n",
    "import logging\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "logging.basicConfig(level=logging.DEBUG,filename='./app.log', filemode='w',format='%(process)d::%(asctime)s::%(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                        ...                                                                       text\n",
       "0   ABBA                        ...                          Look at her face, it's a wonderful face  \\nAnd...\n",
       "1   ABBA                        ...                          Take it easy with me, please  \\nTouch me gentl...\n",
       "2   ABBA                        ...                          I'll never know why I had to go  \\nWhy I had t...\n",
       "3   ABBA                        ...                          Making somebody happy is a question of give an...\n",
       "4   ABBA                        ...                          Making somebody happy is a question of give an...\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = {\n",
    "    \"model\":\"LSTM\",#RNN_TANH, RNN_RELU, LSTM, GRU                         \n",
    "    \"emsize\":300,\n",
    "    \"nhid\":300,\n",
    "    \"nlayers\":3,\n",
    "    \"lr\":20,\n",
    "    \"clip\":0.25,\n",
    "    \"epochs\":40, # upper epoch limit\n",
    "    \"batch_size\":5,\n",
    "    \"bptt\":40,#seq length\n",
    "    \"dropout\":0.2,#dropout applied to layers (0 = no dropout)\n",
    "    \"tied\":True,#'tie the word embedding and softmax weights'\n",
    "    \"seed\":111,\n",
    "    \"log_interval\":200,\n",
    "    \"save\":\"../models/model.pt\"\n",
    "}\n",
    "songdata = pd.read_csv(\"../input/songdata.csv\")\n",
    "songdata.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "914b91d09f320838f9e79f5a5ab976717ee1d906"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>artist</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Donna Summer</th>\n",
       "      <td>191</td>\n",
       "      <td>191</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gordon Lightfoot</th>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bob Dylan</th>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>George Strait</th>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cher</th>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alabama</th>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reba Mcentire</th>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loretta Lynn</th>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dean Martin</th>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chaka Khan</th>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neil Young</th>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hank Williams Jr.</th>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>America</th>\n",
       "      <td>184</td>\n",
       "      <td>184</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nazareth</th>\n",
       "      <td>184</td>\n",
       "      <td>184</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cliff Richard</th>\n",
       "      <td>184</td>\n",
       "      <td>184</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indigo Girls</th>\n",
       "      <td>184</td>\n",
       "      <td>184</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kiss</th>\n",
       "      <td>183</td>\n",
       "      <td>183</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Johnny Cash</th>\n",
       "      <td>183</td>\n",
       "      <td>183</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chris Rea</th>\n",
       "      <td>182</td>\n",
       "      <td>182</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bon Jovi</th>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fleetwood Mac</th>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dolly Parton</th>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rolling Stones</th>\n",
       "      <td>179</td>\n",
       "      <td>179</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Deep Purple</th>\n",
       "      <td>179</td>\n",
       "      <td>179</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roy Orbison</th>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Beatles</th>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rod Stewart</th>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Randy Travis</th>\n",
       "      <td>177</td>\n",
       "      <td>177</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iggy Pop</th>\n",
       "      <td>177</td>\n",
       "      <td>177</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>James Taylor</th>\n",
       "      <td>177</td>\n",
       "      <td>177</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Independence Day</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Side A</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Walk The Moon</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yukmouth</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yonder Mountain String Band</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quarashi</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Koes Plus</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Next To Normal</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Xentrix</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yung Joc</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yeng Constantino</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qntal</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Joseph And The Amazing Technicolor Dreamcoat</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X-Raided</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Young Dro</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gipsy Kings</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Van Der Graaf Generator</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unknown</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exo</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eppu Normaali</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Xavier Naidoo</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Various Artists</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soundtracks</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zazie</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ungu</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exo-K</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U-Kiss</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X-Treme</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zoe</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zed</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>643 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              song  link  text\n",
       "artist                                                        \n",
       "Donna Summer                                   191   191   191\n",
       "Gordon Lightfoot                               189   189   189\n",
       "Bob Dylan                                      188   188   188\n",
       "George Strait                                  188   188   188\n",
       "Cher                                           187   187   187\n",
       "Alabama                                        187   187   187\n",
       "Reba Mcentire                                  187   187   187\n",
       "Loretta Lynn                                   187   187   187\n",
       "Dean Martin                                    186   186   186\n",
       "Chaka Khan                                     186   186   186\n",
       "Neil Young                                     185   185   185\n",
       "Hank Williams Jr.                              185   185   185\n",
       "America                                        184   184   184\n",
       "Nazareth                                       184   184   184\n",
       "Cliff Richard                                  184   184   184\n",
       "Indigo Girls                                   184   184   184\n",
       "Kiss                                           183   183   183\n",
       "Johnny Cash                                    183   183   183\n",
       "Chris Rea                                      182   182   182\n",
       "Bon Jovi                                       181   181   181\n",
       "Fleetwood Mac                                  180   180   180\n",
       "Dolly Parton                                   180   180   180\n",
       "Rolling Stones                                 179   179   179\n",
       "Deep Purple                                    179   179   179\n",
       "Roy Orbison                                    178   178   178\n",
       "The Beatles                                    178   178   178\n",
       "Rod Stewart                                    178   178   178\n",
       "Randy Travis                                   177   177   177\n",
       "Iggy Pop                                       177   177   177\n",
       "James Taylor                                   177   177   177\n",
       "...                                            ...   ...   ...\n",
       "Independence Day                                11    11    11\n",
       "Side A                                          11    11    11\n",
       "Walk The Moon                                   11    11    11\n",
       "Yukmouth                                        11    11    11\n",
       "Yonder Mountain String Band                     10    10    10\n",
       "Quarashi                                        10    10    10\n",
       "Koes Plus                                       10    10    10\n",
       "Next To Normal                                   9     9     9\n",
       "Xentrix                                          9     9     9\n",
       "Yung Joc                                         9     9     9\n",
       "Yeng Constantino                                 8     8     8\n",
       "Qntal                                            8     8     8\n",
       "Joseph And The Amazing Technicolor Dreamcoat     8     8     8\n",
       "X-Raided                                         7     7     7\n",
       "Young Dro                                        7     7     7\n",
       "Gipsy Kings                                      5     5     5\n",
       "Van Der Graaf Generator                          5     5     5\n",
       "Unknown                                          4     4     4\n",
       "Exo                                              3     3     3\n",
       "Eppu Normaali                                    3     3     3\n",
       "Xavier Naidoo                                    3     3     3\n",
       "Various Artists                                  3     3     3\n",
       "Soundtracks                                      3     3     3\n",
       "Zazie                                            2     2     2\n",
       "Ungu                                             2     2     2\n",
       "Exo-K                                            2     2     2\n",
       "U-Kiss                                           1     1     1\n",
       "X-Treme                                          1     1     1\n",
       "Zoe                                              1     1     1\n",
       "Zed                                              1     1     1\n",
       "\n",
       "[643 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                        ...                                                                       text\n",
       "0   ABBA                        ...                          Look at her face, it's a wonderful face  \\nAnd...\n",
       "1   ABBA                        ...                          Take it easy with me, please  \\nTouch me gentl...\n",
       "2   ABBA                        ...                          I'll never know why I had to go  \\nWhy I had t...\n",
       "3   ABBA                        ...                          Making somebody happy is a question of give an...\n",
       "4   ABBA                        ...                          Making somebody happy is a question of give an...\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(songdata.groupby(\"artist\").count().sort_values(by=['song'],ascending=False))\n",
    "subset=songdata[songdata['artist'].isin([\"ABBA\",\"Donna Summer\",\"Gordon Lightfoot\",\n",
    "                                  \"Rolling Stones\",\"Bob Dylan\", \"Iggy Pop\",\"The Beatles\",\n",
    "                                        \"Cher\",\"Bon Jovi\",\"Michael Jackson\",\"Green Day\",\n",
    "                                        \"Red Hot Chili Peppers\",\"Aerosmith\",\"Paul McCartney\",\n",
    "                                        \"Elvis Presley\",\"Robbie Williams\",\"Backstreet Boys\",\n",
    "                                        \"Queen\",\"Mariah Carey\"])]\n",
    "subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "4b2caa16dec7d7443fdf015893356a5c139085ef"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        if rnn_type in ['LSTM', 'GRU']:\n",
    "            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
    "        else:\n",
    "            try:\n",
    "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
    "            except KeyError:\n",
    "                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n",
    "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n",
    "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        # Optionally tie weights as in:\n",
    "        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
    "        # https://arxiv.org/abs/1608.05859\n",
    "        # and\n",
    "        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
    "        # https://arxiv.org/abs/1611.01462\n",
    "        if tie_weights:\n",
    "            if nhid != ninp:\n",
    "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
    "                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
    "        else:\n",
    "            return weight.new_zeros(self.nlayers, bsz, self.nhid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "75f3c521b27f3e36a7f2fd397b7ddc57256aaa07"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dictionary = Dictionary() \n",
    "        lyrics = dataframe['text'].apply(self.pre_process)\n",
    "        train, test, y_train, y_test = train_test_split(lyrics, dataframe['artist'], test_size=0.15, random_state=13)\n",
    "        train, val, y_train, y_val = train_test_split(train, y_train, test_size=0.15, random_state=13)\n",
    "        self.train = self.tokenize(train.str.cat(sep=' end_song '))\n",
    "        self.valid = self.tokenize(val.str.cat(sep=' end_song '))\n",
    "        self.test = self.tokenize(test.str.cat(sep=' end_song '))\n",
    "        self.train_raw = train.str.cat(sep=' end_song ')\n",
    "        self.valid_raw = val.str.cat(sep=' end_song ')\n",
    "        self.test_raw = test.str.cat(sep=' end_song ')\n",
    "        \n",
    "    def pre_process(self,text):\n",
    "        text = text.replace(\"\\r\",\" \")\n",
    "        text = text.replace(\"\\n\",\" \")\n",
    "        text = text.lower()\n",
    "        table = str.maketrans('', '', '!\"#$%&\\()*+-/:;<=>?@[\\\\]^_`{|}~')\n",
    "        text = text.translate(table)\n",
    "        return(text)\n",
    "    \n",
    "    def tokenize(self, string):\n",
    "        tokens = 0\n",
    "        words = nltk.word_tokenize(string)\n",
    "        tokens += len(words)\n",
    "        for word in words:\n",
    "            self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        ids = torch.LongTensor(tokens)\n",
    "        token = 0\n",
    "        words = nltk.word_tokenize(string)\n",
    "        for word in words:\n",
    "            ids[token] = self.dictionary.word2idx[word]\n",
    "            token += 1\n",
    "\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "a3e22aa15396b8a678e8c023fd0de57eaa53fd09"
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Load data\n",
    "###############################################################################\n",
    "subset=songdata[songdata['artist'].isin([\"ABBA\",\"Donna Summer\",\"Gordon Lightfoot\",\n",
    "                                  \"Rolling Stones\",\"Bob Dylan\", \"Iggy Pop\",\"The Beatles\",\n",
    "                                        \"Cher\",\"Bon Jovi\",\"Michael Jackson\",\"Green Day\",\n",
    "                                        \"Red Hot Chili Peppers\",\"Aerosmith\",\"Paul McCartney\",\n",
    "                                        \"Elvis Presley\",\"Robbie Williams\",\"Backstreet Boys\",\n",
    "                                        \"Queen\",\"Mariah Carey\"])]\n",
    "#songdata.groupby(\"artist\").count().sort_values(by=['song'],ascending=False)\n",
    "corpus = Corpus(subset)\n",
    "f = open('../models/corpus.pkl', 'wb')\n",
    "pickle.dump(corpus,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "0719a4d043bfc068bb291637146e7d99a0db5458"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size:5\n",
      "batches:120235\n",
      "batch_size:10\n",
      "batches:10651\n",
      "batch_size:10\n",
      "batches:12648\n"
     ]
    }
   ],
   "source": [
    "# Starting from sequential data, batchify arranges the dataset into columns.\n",
    "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
    "# ┌ a g m s ┐\n",
    "# │ b h n t │\n",
    "# │ c i o u │\n",
    "# │ d j p v │\n",
    "# │ e k q w │\n",
    "# └ f l r x ┘.\n",
    "# These columns are treated as independent by the model, which means that the\n",
    "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
    "# batch processing.\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    print(\"batch_size:\"+str(bsz))\n",
    "    print(\"batches:\"+str(nbatch))\n",
    "    \n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(corpus.train, args['batch_size'])\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "7dce53bb5cca178c9e85dd6046c55a47c09c5667"
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Build the model\n",
    "###############################################################################\n",
    "\n",
    "ntokens = len(corpus.dictionary)\n",
    "model = RNNModel(args['model'], ntokens, args['emsize'], args['nhid'], args['nlayers'], args['dropout'], args['tied']).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "c5290f23d690cb9d7f94743eed0dca575ce27ea8"
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Training code\n",
    "###############################################################################\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "\n",
    "\n",
    "# get_batch subdivides the source data into chunks of length args.bptt.\n",
    "# If source is equal to the example output of the batchify function, with\n",
    "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
    "# ┌ a g m s ┐ ┌ b h n t ┐\n",
    "# └ b h n t ┘ └ c i o u ┘\n",
    "# Note that despite the name of the function, the subdivison of data is not\n",
    "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
    "# by the batchify function. The chunks are along dimension 0, corresponding\n",
    "# to the seq_len dimension in the LSTM.\n",
    "\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(args['bptt'], len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target\n",
    "\n",
    "\n",
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, args['bptt']):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output, hidden = model(data, hidden)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "            hidden = repackage_hidden(hidden)\n",
    "    return total_loss / (len(data_source) - 1)\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "    \n",
    "def train(epoch):\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(args['batch_size'])\n",
    "    cur_loss = 0\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, args['bptt'])):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        \n",
    "        # Reset the gradient after every epoch. \n",
    "        #optimizer.zero_grad()      \n",
    "        model.zero_grad()\n",
    "        \n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimizer take a step and update the weights.\n",
    "        #optimizer.step()\n",
    "        \n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args['clip'])\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % args['log_interval'] == 0 and batch > 0:\n",
    "            cur_loss = total_loss / args['log_interval']\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // args['bptt'], lr,\n",
    "                elapsed * 1000 / args['log_interval'], cur_loss, 2**(cur_loss)))\n",
    "            logging.debug('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.5f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // args['bptt'], lr,\n",
    "                elapsed * 1000 / args['log_interval'], cur_loss, 2**(cur_loss)))\n",
    "            total_loss = 0\n",
    "\n",
    "            start_time = time.time()\n",
    "    for name, param in model.named_parameters():\n",
    "        writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
    "    writer.add_scalar('Train/Loss', cur_loss, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "27af4f077dc779c36cd2a6ad7e25d07ed15eae15",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 3005 batches | lr 20.00 | ms/batch 14.36 | loss  6.85 | ppl   115.29\n",
      "| epoch   1 |   400/ 3005 batches | lr 20.00 | ms/batch 13.85 | loss  6.27 | ppl    77.36\n",
      "| epoch   1 |   600/ 3005 batches | lr 20.00 | ms/batch 13.91 | loss  5.92 | ppl    60.41\n",
      "| epoch   1 |   800/ 3005 batches | lr 20.00 | ms/batch 13.97 | loss  5.62 | ppl    49.19\n",
      "| epoch   1 |  1000/ 3005 batches | lr 20.00 | ms/batch 13.99 | loss  5.39 | ppl    42.07\n",
      "| epoch   1 |  1200/ 3005 batches | lr 20.00 | ms/batch 14.01 | loss  5.39 | ppl    41.81\n",
      "| epoch   1 |  1400/ 3005 batches | lr 20.00 | ms/batch 14.00 | loss  5.25 | ppl    38.17\n",
      "| epoch   1 |  1600/ 3005 batches | lr 20.00 | ms/batch 14.00 | loss  5.10 | ppl    34.25\n",
      "| epoch   1 |  1800/ 3005 batches | lr 20.00 | ms/batch 14.03 | loss  5.17 | ppl    36.06\n",
      "| epoch   1 |  2000/ 3005 batches | lr 20.00 | ms/batch 14.04 | loss  5.08 | ppl    33.74\n",
      "| epoch   1 |  2200/ 3005 batches | lr 20.00 | ms/batch 14.07 | loss  5.01 | ppl    32.21\n",
      "| epoch   1 |  2400/ 3005 batches | lr 20.00 | ms/batch 14.21 | loss  4.97 | ppl    31.41\n",
      "| epoch   1 |  2600/ 3005 batches | lr 20.00 | ms/batch 14.12 | loss  4.99 | ppl    31.73\n",
      "| epoch   1 |  2800/ 3005 batches | lr 20.00 | ms/batch 14.05 | loss  4.99 | ppl    31.74\n",
      "| epoch   1 |  3000/ 3005 batches | lr 20.00 | ms/batch 14.11 | loss  4.76 | ppl    27.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 43.83s | valid loss  5.30 | valid ppl    39.27\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type RNNModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |   200/ 3005 batches | lr 20.00 | ms/batch 14.19 | loss  4.88 | ppl    29.36\n",
      "| epoch   2 |   400/ 3005 batches | lr 20.00 | ms/batch 14.07 | loss  4.82 | ppl    28.34\n",
      "| epoch   2 |   600/ 3005 batches | lr 20.00 | ms/batch 14.02 | loss  4.79 | ppl    27.69\n",
      "| epoch   2 |   800/ 3005 batches | lr 20.00 | ms/batch 14.10 | loss  4.80 | ppl    27.92\n",
      "| epoch   2 |  1000/ 3005 batches | lr 20.00 | ms/batch 14.06 | loss  4.75 | ppl    26.95\n",
      "| epoch   2 |  1200/ 3005 batches | lr 20.00 | ms/batch 14.04 | loss  4.81 | ppl    28.09\n",
      "| epoch   2 |  1400/ 3005 batches | lr 20.00 | ms/batch 14.00 | loss  4.74 | ppl    26.79\n",
      "| epoch   2 |  1600/ 3005 batches | lr 20.00 | ms/batch 14.03 | loss  4.66 | ppl    25.30\n",
      "| epoch   2 |  1800/ 3005 batches | lr 20.00 | ms/batch 14.06 | loss  4.76 | ppl    27.01\n",
      "| epoch   2 |  2000/ 3005 batches | lr 20.00 | ms/batch 14.07 | loss  4.71 | ppl    26.18\n",
      "| epoch   2 |  2200/ 3005 batches | lr 20.00 | ms/batch 14.04 | loss  4.66 | ppl    25.32\n",
      "| epoch   2 |  2400/ 3005 batches | lr 20.00 | ms/batch 14.02 | loss  4.63 | ppl    24.71\n",
      "| epoch   2 |  2600/ 3005 batches | lr 20.00 | ms/batch 13.97 | loss  4.66 | ppl    25.36\n",
      "| epoch   2 |  2800/ 3005 batches | lr 20.00 | ms/batch 13.95 | loss  4.68 | ppl    25.61\n",
      "| epoch   2 |  3000/ 3005 batches | lr 20.00 | ms/batch 13.99 | loss  4.45 | ppl    21.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 43.74s | valid loss  5.08 | valid ppl    33.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 3005 batches | lr 20.00 | ms/batch 14.05 | loss  4.60 | ppl    24.22\n",
      "| epoch   3 |   400/ 3005 batches | lr 20.00 | ms/batch 13.98 | loss  4.57 | ppl    23.71\n",
      "| epoch   3 |   600/ 3005 batches | lr 20.00 | ms/batch 14.02 | loss  4.52 | ppl    22.93\n",
      "| epoch   3 |   800/ 3005 batches | lr 20.00 | ms/batch 13.85 | loss  4.55 | ppl    23.39\n",
      "| epoch   3 |  1000/ 3005 batches | lr 20.00 | ms/batch 13.97 | loss  4.53 | ppl    23.16\n",
      "| epoch   3 |  1200/ 3005 batches | lr 20.00 | ms/batch 14.01 | loss  4.58 | ppl    23.93\n",
      "| epoch   3 |  1400/ 3005 batches | lr 20.00 | ms/batch 14.05 | loss  4.53 | ppl    23.07\n",
      "| epoch   3 |  1600/ 3005 batches | lr 20.00 | ms/batch 13.95 | loss  4.48 | ppl    22.26\n",
      "| epoch   3 |  1800/ 3005 batches | lr 20.00 | ms/batch 14.03 | loss  4.55 | ppl    23.49\n",
      "| epoch   3 |  2000/ 3005 batches | lr 20.00 | ms/batch 13.93 | loss  4.52 | ppl    22.89\n",
      "| epoch   3 |  2200/ 3005 batches | lr 20.00 | ms/batch 13.99 | loss  4.48 | ppl    22.29\n",
      "| epoch   3 |  2400/ 3005 batches | lr 20.00 | ms/batch 13.96 | loss  4.44 | ppl    21.66\n",
      "| epoch   3 |  2600/ 3005 batches | lr 20.00 | ms/batch 13.97 | loss  4.47 | ppl    22.18\n",
      "| epoch   3 |  2800/ 3005 batches | lr 20.00 | ms/batch 13.99 | loss  4.48 | ppl    22.36\n",
      "| epoch   3 |  3000/ 3005 batches | lr 20.00 | ms/batch 13.94 | loss  4.28 | ppl    19.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 43.60s | valid loss  5.00 | valid ppl    32.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 3005 batches | lr 20.00 | ms/batch 14.07 | loss  4.44 | ppl    21.75\n",
      "| epoch   4 |   400/ 3005 batches | lr 20.00 | ms/batch 13.97 | loss  4.40 | ppl    21.13\n",
      "| epoch   4 |   600/ 3005 batches | lr 20.00 | ms/batch 13.92 | loss  4.35 | ppl    20.39\n",
      "| epoch   4 |   800/ 3005 batches | lr 20.00 | ms/batch 13.89 | loss  4.38 | ppl    20.87\n",
      "| epoch   4 |  1000/ 3005 batches | lr 20.00 | ms/batch 13.93 | loss  4.38 | ppl    20.88\n",
      "| epoch   4 |  1200/ 3005 batches | lr 20.00 | ms/batch 13.64 | loss  4.42 | ppl    21.40\n",
      "| epoch   4 |  1400/ 3005 batches | lr 20.00 | ms/batch 13.42 | loss  4.37 | ppl    20.75\n",
      "| epoch   4 |  1600/ 3005 batches | lr 20.00 | ms/batch 13.41 | loss  4.34 | ppl    20.29\n",
      "| epoch   4 |  1800/ 3005 batches | lr 20.00 | ms/batch 13.40 | loss  4.42 | ppl    21.40\n",
      "| epoch   4 |  2000/ 3005 batches | lr 20.00 | ms/batch 13.84 | loss  4.38 | ppl    20.84\n",
      "| epoch   4 |  2200/ 3005 batches | lr 20.00 | ms/batch 14.02 | loss  4.35 | ppl    20.33\n",
      "| epoch   4 |  2400/ 3005 batches | lr 20.00 | ms/batch 13.94 | loss  4.29 | ppl    19.55\n",
      "| epoch   4 |  2600/ 3005 batches | lr 20.00 | ms/batch 13.95 | loss  4.33 | ppl    20.17\n",
      "| epoch   4 |  2800/ 3005 batches | lr 20.00 | ms/batch 13.99 | loss  4.35 | ppl    20.34\n",
      "| epoch   4 |  3000/ 3005 batches | lr 20.00 | ms/batch 13.99 | loss  4.15 | ppl    17.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 43.14s | valid loss  4.97 | valid ppl    31.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 3005 batches | lr 20.00 | ms/batch 14.06 | loss  4.30 | ppl    19.76\n",
      "| epoch   5 |   400/ 3005 batches | lr 20.00 | ms/batch 13.97 | loss  4.28 | ppl    19.37\n",
      "| epoch   5 |   600/ 3005 batches | lr 20.00 | ms/batch 13.97 | loss  4.22 | ppl    18.59\n",
      "| epoch   5 |   800/ 3005 batches | lr 20.00 | ms/batch 14.01 | loss  4.26 | ppl    19.16\n",
      "| epoch   5 |  1000/ 3005 batches | lr 20.00 | ms/batch 14.02 | loss  4.26 | ppl    19.22\n",
      "| epoch   5 |  1200/ 3005 batches | lr 20.00 | ms/batch 14.05 | loss  4.29 | ppl    19.57\n",
      "| epoch   5 |  1400/ 3005 batches | lr 20.00 | ms/batch 13.99 | loss  4.26 | ppl    19.11\n",
      "| epoch   5 |  1600/ 3005 batches | lr 20.00 | ms/batch 14.01 | loss  4.23 | ppl    18.80\n",
      "| epoch   5 |  1800/ 3005 batches | lr 20.00 | ms/batch 14.00 | loss  4.31 | ppl    19.78\n",
      "| epoch   5 |  2000/ 3005 batches | lr 20.00 | ms/batch 14.00 | loss  4.28 | ppl    19.37\n",
      "| epoch   5 |  2200/ 3005 batches | lr 20.00 | ms/batch 14.05 | loss  4.22 | ppl    18.69\n",
      "| epoch   5 |  2400/ 3005 batches | lr 20.00 | ms/batch 14.04 | loss  4.19 | ppl    18.28\n",
      "| epoch   5 |  2600/ 3005 batches | lr 20.00 | ms/batch 14.00 | loss  4.22 | ppl    18.63\n",
      "| epoch   5 |  2800/ 3005 batches | lr 20.00 | ms/batch 14.03 | loss  4.25 | ppl    19.05\n",
      "| epoch   5 |  3000/ 3005 batches | lr 20.00 | ms/batch 14.00 | loss  4.04 | ppl    16.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 43.69s | valid loss  4.94 | valid ppl    30.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/ 3005 batches | lr 20.00 | ms/batch 14.16 | loss  4.21 | ppl    18.45\n",
      "| epoch   6 |   400/ 3005 batches | lr 20.00 | ms/batch 14.05 | loss  4.18 | ppl    18.09\n",
      "| epoch   6 |   600/ 3005 batches | lr 20.00 | ms/batch 14.12 | loss  4.11 | ppl    17.30\n",
      "| epoch   6 |   800/ 3005 batches | lr 20.00 | ms/batch 14.04 | loss  4.17 | ppl    17.96\n",
      "| epoch   6 |  1000/ 3005 batches | lr 20.00 | ms/batch 13.58 | loss  4.16 | ppl    17.86\n",
      "| epoch   6 |  1200/ 3005 batches | lr 20.00 | ms/batch 13.39 | loss  4.19 | ppl    18.20\n",
      "| epoch   6 |  1400/ 3005 batches | lr 20.00 | ms/batch 13.84 | loss  4.16 | ppl    17.88\n",
      "| epoch   6 |  1600/ 3005 batches | lr 20.00 | ms/batch 13.94 | loss  4.16 | ppl    17.86\n",
      "| epoch   6 |  1800/ 3005 batches | lr 20.00 | ms/batch 13.97 | loss  4.22 | ppl    18.69\n",
      "| epoch   6 |  2000/ 3005 batches | lr 20.00 | ms/batch 14.01 | loss  4.19 | ppl    18.29\n",
      "| epoch   6 |  2200/ 3005 batches | lr 20.00 | ms/batch 14.03 | loss  4.14 | ppl    17.59\n",
      "| epoch   6 |  2400/ 3005 batches | lr 20.00 | ms/batch 14.03 | loss  4.09 | ppl    17.07\n",
      "| epoch   6 |  2600/ 3005 batches | lr 20.00 | ms/batch 14.01 | loss  4.13 | ppl    17.56\n",
      "| epoch   6 |  2800/ 3005 batches | lr 20.00 | ms/batch 13.99 | loss  4.16 | ppl    17.88\n",
      "| epoch   6 |  3000/ 3005 batches | lr 20.00 | ms/batch 14.02 | loss  3.95 | ppl    15.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 43.48s | valid loss  4.95 | valid ppl    30.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   200/ 3005 batches | lr 5.00 | ms/batch 14.09 | loss  4.31 | ppl    19.77\n",
      "| epoch   7 |   400/ 3005 batches | lr 5.00 | ms/batch 14.00 | loss  4.22 | ppl    18.61\n",
      "| epoch   7 |   600/ 3005 batches | lr 5.00 | ms/batch 14.01 | loss  4.15 | ppl    17.80\n",
      "| epoch   7 |   800/ 3005 batches | lr 5.00 | ms/batch 13.98 | loss  4.16 | ppl    17.91\n",
      "| epoch   7 |  1000/ 3005 batches | lr 5.00 | ms/batch 14.00 | loss  4.14 | ppl    17.67\n",
      "| epoch   7 |  1200/ 3005 batches | lr 5.00 | ms/batch 13.98 | loss  4.14 | ppl    17.63\n",
      "| epoch   7 |  1400/ 3005 batches | lr 5.00 | ms/batch 14.02 | loss  4.11 | ppl    17.31\n",
      "| epoch   7 |  1600/ 3005 batches | lr 5.00 | ms/batch 14.07 | loss  4.05 | ppl    16.52\n",
      "| epoch   7 |  1800/ 3005 batches | lr 5.00 | ms/batch 14.08 | loss  4.09 | ppl    17.05\n",
      "| epoch   7 |  2000/ 3005 batches | lr 5.00 | ms/batch 14.07 | loss  4.05 | ppl    16.59\n",
      "| epoch   7 |  2200/ 3005 batches | lr 5.00 | ms/batch 14.05 | loss  3.97 | ppl    15.67\n",
      "| epoch   7 |  2400/ 3005 batches | lr 5.00 | ms/batch 14.05 | loss  3.90 | ppl    14.94\n",
      "| epoch   7 |  2600/ 3005 batches | lr 5.00 | ms/batch 14.28 | loss  3.91 | ppl    15.06\n",
      "| epoch   7 |  2800/ 3005 batches | lr 5.00 | ms/batch 14.08 | loss  3.91 | ppl    15.02\n",
      "| epoch   7 |  3000/ 3005 batches | lr 5.00 | ms/batch 14.07 | loss  3.66 | ppl    12.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 43.83s | valid loss  4.80 | valid ppl    27.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   200/ 3005 batches | lr 5.00 | ms/batch 14.12 | loss  4.07 | ppl    16.77\n",
      "| epoch   8 |   400/ 3005 batches | lr 5.00 | ms/batch 14.07 | loss  4.01 | ppl    16.07\n",
      "| epoch   8 |   600/ 3005 batches | lr 5.00 | ms/batch 14.09 | loss  3.95 | ppl    15.40\n",
      "| epoch   8 |   800/ 3005 batches | lr 5.00 | ms/batch 14.00 | loss  3.99 | ppl    15.88\n",
      "| epoch   8 |  1000/ 3005 batches | lr 5.00 | ms/batch 14.04 | loss  3.99 | ppl    15.88\n",
      "| epoch   8 |  1200/ 3005 batches | lr 5.00 | ms/batch 13.97 | loss  3.98 | ppl    15.73\n",
      "| epoch   8 |  1400/ 3005 batches | lr 5.00 | ms/batch 13.98 | loss  3.97 | ppl    15.62\n",
      "| epoch   8 |  1600/ 3005 batches | lr 5.00 | ms/batch 14.03 | loss  3.93 | ppl    15.24\n",
      "| epoch   8 |  1800/ 3005 batches | lr 5.00 | ms/batch 14.10 | loss  3.98 | ppl    15.76\n",
      "| epoch   8 |  2000/ 3005 batches | lr 5.00 | ms/batch 14.09 | loss  3.95 | ppl    15.40\n",
      "| epoch   8 |  2200/ 3005 batches | lr 5.00 | ms/batch 14.12 | loss  3.86 | ppl    14.56\n",
      "| epoch   8 |  2400/ 3005 batches | lr 5.00 | ms/batch 14.08 | loss  3.81 | ppl    14.03\n",
      "| epoch   8 |  2600/ 3005 batches | lr 5.00 | ms/batch 14.08 | loss  3.83 | ppl    14.25\n",
      "| epoch   8 |  2800/ 3005 batches | lr 5.00 | ms/batch 14.11 | loss  3.83 | ppl    14.23\n",
      "| epoch   8 |  3000/ 3005 batches | lr 5.00 | ms/batch 14.05 | loss  3.61 | ppl    12.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 43.85s | valid loss  4.79 | valid ppl    27.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   200/ 3005 batches | lr 5.00 | ms/batch 14.14 | loss  3.96 | ppl    15.59\n",
      "| epoch   9 |   400/ 3005 batches | lr 5.00 | ms/batch 14.14 | loss  3.91 | ppl    15.04\n",
      "| epoch   9 |   600/ 3005 batches | lr 5.00 | ms/batch 14.02 | loss  3.85 | ppl    14.45\n",
      "| epoch   9 |   800/ 3005 batches | lr 5.00 | ms/batch 14.00 | loss  3.90 | ppl    14.89\n",
      "| epoch   9 |  1000/ 3005 batches | lr 5.00 | ms/batch 14.02 | loss  3.89 | ppl    14.84\n",
      "| epoch   9 |  1200/ 3005 batches | lr 5.00 | ms/batch 14.09 | loss  3.88 | ppl    14.71\n",
      "| epoch   9 |  1400/ 3005 batches | lr 5.00 | ms/batch 14.00 | loss  3.88 | ppl    14.75\n",
      "| epoch   9 |  1600/ 3005 batches | lr 5.00 | ms/batch 14.06 | loss  3.86 | ppl    14.50\n",
      "| epoch   9 |  1800/ 3005 batches | lr 5.00 | ms/batch 13.99 | loss  3.90 | ppl    14.94\n",
      "| epoch   9 |  2000/ 3005 batches | lr 5.00 | ms/batch 14.09 | loss  3.88 | ppl    14.68\n",
      "| epoch   9 |  2200/ 3005 batches | lr 5.00 | ms/batch 14.03 | loss  3.80 | ppl    13.93\n",
      "| epoch   9 |  2400/ 3005 batches | lr 5.00 | ms/batch 14.06 | loss  3.75 | ppl    13.41\n",
      "| epoch   9 |  2600/ 3005 batches | lr 5.00 | ms/batch 14.05 | loss  3.76 | ppl    13.54\n",
      "| epoch   9 |  2800/ 3005 batches | lr 5.00 | ms/batch 14.05 | loss  3.78 | ppl    13.70\n",
      "| epoch   9 |  3000/ 3005 batches | lr 5.00 | ms/batch 14.04 | loss  3.55 | ppl    11.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 43.80s | valid loss  4.79 | valid ppl    27.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   200/ 3005 batches | lr 1.25 | ms/batch 14.12 | loss  3.98 | ppl    15.77\n",
      "| epoch  10 |   400/ 3005 batches | lr 1.25 | ms/batch 14.07 | loss  3.93 | ppl    15.28\n",
      "| epoch  10 |   600/ 3005 batches | lr 1.25 | ms/batch 14.05 | loss  3.88 | ppl    14.68\n",
      "| epoch  10 |   800/ 3005 batches | lr 1.25 | ms/batch 14.07 | loss  3.91 | ppl    15.03\n",
      "| epoch  10 |  1000/ 3005 batches | lr 1.25 | ms/batch 14.06 | loss  3.90 | ppl    14.91\n",
      "| epoch  10 |  1200/ 3005 batches | lr 1.25 | ms/batch 14.02 | loss  3.88 | ppl    14.72\n",
      "| epoch  10 |  1400/ 3005 batches | lr 1.25 | ms/batch 14.06 | loss  3.86 | ppl    14.54\n",
      "| epoch  10 |  1600/ 3005 batches | lr 1.25 | ms/batch 14.04 | loss  3.83 | ppl    14.20\n",
      "| epoch  10 |  1800/ 3005 batches | lr 1.25 | ms/batch 14.07 | loss  3.86 | ppl    14.55\n",
      "| epoch  10 |  2000/ 3005 batches | lr 1.25 | ms/batch 14.04 | loss  3.82 | ppl    14.16\n",
      "| epoch  10 |  2200/ 3005 batches | lr 1.25 | ms/batch 14.05 | loss  3.73 | ppl    13.31\n",
      "| epoch  10 |  2400/ 3005 batches | lr 1.25 | ms/batch 14.07 | loss  3.68 | ppl    12.81\n",
      "| epoch  10 |  2600/ 3005 batches | lr 1.25 | ms/batch 14.06 | loss  3.70 | ppl    12.99\n",
      "| epoch  10 |  2800/ 3005 batches | lr 1.25 | ms/batch 14.05 | loss  3.71 | ppl    13.05\n",
      "| epoch  10 |  3000/ 3005 batches | lr 1.25 | ms/batch 14.00 | loss  3.46 | ppl    11.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 43.85s | valid loss  4.75 | valid ppl    26.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |   200/ 3005 batches | lr 1.25 | ms/batch 14.01 | loss  3.91 | ppl    15.06\n",
      "| epoch  11 |   400/ 3005 batches | lr 1.25 | ms/batch 13.93 | loss  3.86 | ppl    14.57\n",
      "| epoch  11 |   600/ 3005 batches | lr 1.25 | ms/batch 13.98 | loss  3.80 | ppl    13.91\n",
      "| epoch  11 |   800/ 3005 batches | lr 1.25 | ms/batch 14.00 | loss  3.86 | ppl    14.48\n",
      "| epoch  11 |  1000/ 3005 batches | lr 1.25 | ms/batch 13.99 | loss  3.83 | ppl    14.22\n",
      "| epoch  11 |  1200/ 3005 batches | lr 1.25 | ms/batch 13.96 | loss  3.83 | ppl    14.19\n",
      "| epoch  11 |  1400/ 3005 batches | lr 1.25 | ms/batch 14.00 | loss  3.82 | ppl    14.11\n",
      "| epoch  11 |  1600/ 3005 batches | lr 1.25 | ms/batch 13.42 | loss  3.79 | ppl    13.82\n",
      "| epoch  11 |  1800/ 3005 batches | lr 1.25 | ms/batch 13.40 | loss  3.83 | ppl    14.19\n",
      "| epoch  11 |  2000/ 3005 batches | lr 1.25 | ms/batch 13.41 | loss  3.80 | ppl    13.94\n",
      "| epoch  11 |  2200/ 3005 batches | lr 1.25 | ms/batch 13.52 | loss  3.71 | ppl    13.07\n",
      "| epoch  11 |  2400/ 3005 batches | lr 1.25 | ms/batch 13.96 | loss  3.65 | ppl    12.56\n",
      "| epoch  11 |  2600/ 3005 batches | lr 1.25 | ms/batch 13.99 | loss  3.68 | ppl    12.81\n",
      "| epoch  11 |  2800/ 3005 batches | lr 1.25 | ms/batch 14.01 | loss  3.69 | ppl    12.87\n",
      "| epoch  11 |  3000/ 3005 batches | lr 1.25 | ms/batch 13.96 | loss  3.45 | ppl    10.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 43.17s | valid loss  4.74 | valid ppl    26.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |   200/ 3005 batches | lr 1.25 | ms/batch 14.12 | loss  3.88 | ppl    14.71\n",
      "| epoch  12 |   400/ 3005 batches | lr 1.25 | ms/batch 14.00 | loss  3.83 | ppl    14.24\n",
      "| epoch  12 |   600/ 3005 batches | lr 1.25 | ms/batch 13.92 | loss  3.76 | ppl    13.59\n",
      "| epoch  12 |   800/ 3005 batches | lr 1.25 | ms/batch 13.93 | loss  3.82 | ppl    14.11\n",
      "| epoch  12 |  1000/ 3005 batches | lr 1.25 | ms/batch 13.92 | loss  3.81 | ppl    13.99\n",
      "| epoch  12 |  1200/ 3005 batches | lr 1.25 | ms/batch 13.85 | loss  3.80 | ppl    13.96\n",
      "| epoch  12 |  1400/ 3005 batches | lr 1.25 | ms/batch 13.87 | loss  3.79 | ppl    13.79\n",
      "| epoch  12 |  1600/ 3005 batches | lr 1.25 | ms/batch 13.93 | loss  3.76 | ppl    13.56\n",
      "| epoch  12 |  1800/ 3005 batches | lr 1.25 | ms/batch 13.95 | loss  3.80 | ppl    13.89\n",
      "| epoch  12 |  2000/ 3005 batches | lr 1.25 | ms/batch 13.90 | loss  3.78 | ppl    13.70\n",
      "| epoch  12 |  2200/ 3005 batches | lr 1.25 | ms/batch 13.90 | loss  3.69 | ppl    12.90\n",
      "| epoch  12 |  2400/ 3005 batches | lr 1.25 | ms/batch 13.94 | loss  3.64 | ppl    12.44\n",
      "| epoch  12 |  2600/ 3005 batches | lr 1.25 | ms/batch 13.98 | loss  3.67 | ppl    12.69\n",
      "| epoch  12 |  2800/ 3005 batches | lr 1.25 | ms/batch 13.95 | loss  3.67 | ppl    12.76\n",
      "| epoch  12 |  3000/ 3005 batches | lr 1.25 | ms/batch 13.99 | loss  3.44 | ppl    10.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 43.48s | valid loss  4.75 | valid ppl    26.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |   200/ 3005 batches | lr 0.31 | ms/batch 14.01 | loss  3.89 | ppl    14.80\n",
      "| epoch  13 |   400/ 3005 batches | lr 0.31 | ms/batch 14.01 | loss  3.84 | ppl    14.33\n",
      "| epoch  13 |   600/ 3005 batches | lr 0.31 | ms/batch 13.91 | loss  3.78 | ppl    13.73\n",
      "| epoch  13 |   800/ 3005 batches | lr 0.31 | ms/batch 13.93 | loss  3.83 | ppl    14.21\n",
      "| epoch  13 |  1000/ 3005 batches | lr 0.31 | ms/batch 13.60 | loss  3.82 | ppl    14.11\n",
      "| epoch  13 |  1200/ 3005 batches | lr 0.31 | ms/batch 13.41 | loss  3.81 | ppl    13.99\n",
      "| epoch  13 |  1400/ 3005 batches | lr 0.31 | ms/batch 13.82 | loss  3.79 | ppl    13.86\n",
      "| epoch  13 |  1600/ 3005 batches | lr 0.31 | ms/batch 13.93 | loss  3.76 | ppl    13.55\n",
      "| epoch  13 |  1800/ 3005 batches | lr 0.31 | ms/batch 13.91 | loss  3.80 | ppl    13.93\n",
      "| epoch  13 |  2000/ 3005 batches | lr 0.31 | ms/batch 13.94 | loss  3.76 | ppl    13.51\n",
      "| epoch  13 |  2200/ 3005 batches | lr 0.31 | ms/batch 13.87 | loss  3.67 | ppl    12.69\n",
      "| epoch  13 |  2400/ 3005 batches | lr 0.31 | ms/batch 13.94 | loss  3.61 | ppl    12.20\n",
      "| epoch  13 |  2600/ 3005 batches | lr 0.31 | ms/batch 13.96 | loss  3.63 | ppl    12.39\n",
      "| epoch  13 |  2800/ 3005 batches | lr 0.31 | ms/batch 13.99 | loss  3.65 | ppl    12.55\n",
      "| epoch  13 |  3000/ 3005 batches | lr 0.31 | ms/batch 13.94 | loss  3.40 | ppl    10.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 43.27s | valid loss  4.73 | valid ppl    26.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |   200/ 3005 batches | lr 0.31 | ms/batch 14.02 | loss  3.86 | ppl    14.56\n",
      "| epoch  14 |   400/ 3005 batches | lr 0.31 | ms/batch 13.93 | loss  3.82 | ppl    14.08\n",
      "| epoch  14 |   600/ 3005 batches | lr 0.31 | ms/batch 13.90 | loss  3.76 | ppl    13.53\n",
      "| epoch  14 |   800/ 3005 batches | lr 0.31 | ms/batch 13.86 | loss  3.81 | ppl    14.03\n",
      "| epoch  14 |  1000/ 3005 batches | lr 0.31 | ms/batch 13.91 | loss  3.79 | ppl    13.85\n",
      "| epoch  14 |  1200/ 3005 batches | lr 0.31 | ms/batch 13.96 | loss  3.79 | ppl    13.82\n",
      "| epoch  14 |  1400/ 3005 batches | lr 0.31 | ms/batch 13.99 | loss  3.77 | ppl    13.67\n",
      "| epoch  14 |  1600/ 3005 batches | lr 0.31 | ms/batch 13.90 | loss  3.75 | ppl    13.41\n",
      "| epoch  14 |  1800/ 3005 batches | lr 0.31 | ms/batch 13.88 | loss  3.78 | ppl    13.78\n",
      "| epoch  14 |  2000/ 3005 batches | lr 0.31 | ms/batch 13.87 | loss  3.75 | ppl    13.45\n",
      "| epoch  14 |  2200/ 3005 batches | lr 0.31 | ms/batch 13.94 | loss  3.66 | ppl    12.64\n",
      "| epoch  14 |  2400/ 3005 batches | lr 0.31 | ms/batch 13.91 | loss  3.61 | ppl    12.19\n",
      "| epoch  14 |  2600/ 3005 batches | lr 0.31 | ms/batch 14.04 | loss  3.63 | ppl    12.40\n",
      "| epoch  14 |  2800/ 3005 batches | lr 0.31 | ms/batch 14.02 | loss  3.65 | ppl    12.52\n",
      "| epoch  14 |  3000/ 3005 batches | lr 0.31 | ms/batch 13.93 | loss  3.40 | ppl    10.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 43.42s | valid loss  4.72 | valid ppl    26.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |   200/ 3005 batches | lr 0.31 | ms/batch 14.03 | loss  3.86 | ppl    14.48\n",
      "| epoch  15 |   400/ 3005 batches | lr 0.31 | ms/batch 13.97 | loss  3.81 | ppl    14.02\n",
      "| epoch  15 |   600/ 3005 batches | lr 0.31 | ms/batch 13.95 | loss  3.74 | ppl    13.40\n",
      "| epoch  15 |   800/ 3005 batches | lr 0.31 | ms/batch 13.94 | loss  3.79 | ppl    13.86\n",
      "| epoch  15 |  1000/ 3005 batches | lr 0.31 | ms/batch 13.89 | loss  3.78 | ppl    13.71\n",
      "| epoch  15 |  1200/ 3005 batches | lr 0.31 | ms/batch 13.91 | loss  3.78 | ppl    13.73\n",
      "| epoch  15 |  1400/ 3005 batches | lr 0.31 | ms/batch 14.05 | loss  3.75 | ppl    13.49\n",
      "| epoch  15 |  1600/ 3005 batches | lr 0.31 | ms/batch 14.06 | loss  3.74 | ppl    13.36\n",
      "| epoch  15 |  1800/ 3005 batches | lr 0.31 | ms/batch 14.10 | loss  3.78 | ppl    13.71\n",
      "| epoch  15 |  2000/ 3005 batches | lr 0.31 | ms/batch 14.00 | loss  3.74 | ppl    13.37\n",
      "| epoch  15 |  2200/ 3005 batches | lr 0.31 | ms/batch 13.98 | loss  3.65 | ppl    12.57\n",
      "| epoch  15 |  2400/ 3005 batches | lr 0.31 | ms/batch 13.94 | loss  3.60 | ppl    12.16\n",
      "| epoch  15 |  2600/ 3005 batches | lr 0.31 | ms/batch 13.91 | loss  3.63 | ppl    12.35\n",
      "| epoch  15 |  2800/ 3005 batches | lr 0.31 | ms/batch 13.91 | loss  3.65 | ppl    12.52\n",
      "| epoch  15 |  3000/ 3005 batches | lr 0.31 | ms/batch 13.95 | loss  3.40 | ppl    10.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 43.54s | valid loss  4.72 | valid ppl    26.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |   200/ 3005 batches | lr 0.31 | ms/batch 13.97 | loss  3.85 | ppl    14.43\n",
      "| epoch  16 |   400/ 3005 batches | lr 0.31 | ms/batch 14.08 | loss  3.79 | ppl    13.85\n",
      "| epoch  16 |   600/ 3005 batches | lr 0.31 | ms/batch 13.94 | loss  3.73 | ppl    13.26\n",
      "| epoch  16 |   800/ 3005 batches | lr 0.31 | ms/batch 13.94 | loss  3.79 | ppl    13.82\n",
      "| epoch  16 |  1000/ 3005 batches | lr 0.31 | ms/batch 13.91 | loss  3.77 | ppl    13.68\n",
      "| epoch  16 |  1200/ 3005 batches | lr 0.31 | ms/batch 13.91 | loss  3.76 | ppl    13.57\n",
      "| epoch  16 |  1400/ 3005 batches | lr 0.31 | ms/batch 13.91 | loss  3.75 | ppl    13.50\n",
      "| epoch  16 |  1600/ 3005 batches | lr 0.31 | ms/batch 13.99 | loss  3.73 | ppl    13.28\n",
      "| epoch  16 |  1800/ 3005 batches | lr 0.31 | ms/batch 13.90 | loss  3.76 | ppl    13.58\n",
      "| epoch  16 |  2000/ 3005 batches | lr 0.31 | ms/batch 13.92 | loss  3.74 | ppl    13.36\n",
      "| epoch  16 |  2200/ 3005 batches | lr 0.31 | ms/batch 13.92 | loss  3.65 | ppl    12.55\n",
      "| epoch  16 |  2400/ 3005 batches | lr 0.31 | ms/batch 13.93 | loss  3.60 | ppl    12.15\n",
      "| epoch  16 |  2600/ 3005 batches | lr 0.31 | ms/batch 13.91 | loss  3.63 | ppl    12.37\n",
      "| epoch  16 |  2800/ 3005 batches | lr 0.31 | ms/batch 13.90 | loss  3.65 | ppl    12.51\n",
      "| epoch  16 |  3000/ 3005 batches | lr 0.31 | ms/batch 13.87 | loss  3.40 | ppl    10.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 43.42s | valid loss  4.72 | valid ppl    26.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |   200/ 3005 batches | lr 0.08 | ms/batch 13.97 | loss  3.85 | ppl    14.40\n",
      "| epoch  17 |   400/ 3005 batches | lr 0.08 | ms/batch 13.94 | loss  3.80 | ppl    13.95\n",
      "| epoch  17 |   600/ 3005 batches | lr 0.08 | ms/batch 13.93 | loss  3.73 | ppl    13.28\n",
      "| epoch  17 |   800/ 3005 batches | lr 0.08 | ms/batch 13.92 | loss  3.80 | ppl    13.90\n",
      "| epoch  17 |  1000/ 3005 batches | lr 0.08 | ms/batch 13.93 | loss  3.78 | ppl    13.69\n",
      "| epoch  17 |  1200/ 3005 batches | lr 0.08 | ms/batch 13.94 | loss  3.77 | ppl    13.63\n",
      "| epoch  17 |  1400/ 3005 batches | lr 0.08 | ms/batch 13.98 | loss  3.75 | ppl    13.48\n",
      "| epoch  17 |  1600/ 3005 batches | lr 0.08 | ms/batch 13.98 | loss  3.73 | ppl    13.30\n",
      "| epoch  17 |  1800/ 3005 batches | lr 0.08 | ms/batch 14.02 | loss  3.77 | ppl    13.61\n",
      "| epoch  17 |  2000/ 3005 batches | lr 0.08 | ms/batch 13.96 | loss  3.73 | ppl    13.26\n",
      "| epoch  17 |  2200/ 3005 batches | lr 0.08 | ms/batch 14.01 | loss  3.65 | ppl    12.53\n",
      "| epoch  17 |  2400/ 3005 batches | lr 0.08 | ms/batch 14.01 | loss  3.59 | ppl    12.05\n",
      "| epoch  17 |  2600/ 3005 batches | lr 0.08 | ms/batch 13.93 | loss  3.61 | ppl    12.19\n",
      "| epoch  17 |  2800/ 3005 batches | lr 0.08 | ms/batch 13.93 | loss  3.63 | ppl    12.35\n",
      "| epoch  17 |  3000/ 3005 batches | lr 0.08 | ms/batch 13.93 | loss  3.39 | ppl    10.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 43.49s | valid loss  4.71 | valid ppl    26.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |   200/ 3005 batches | lr 0.08 | ms/batch 14.09 | loss  3.84 | ppl    14.27\n",
      "| epoch  18 |   400/ 3005 batches | lr 0.08 | ms/batch 13.98 | loss  3.79 | ppl    13.86\n",
      "| epoch  18 |   600/ 3005 batches | lr 0.08 | ms/batch 13.98 | loss  3.73 | ppl    13.26\n",
      "| epoch  18 |   800/ 3005 batches | lr 0.08 | ms/batch 13.95 | loss  3.79 | ppl    13.83\n",
      "| epoch  18 |  1000/ 3005 batches | lr 0.08 | ms/batch 13.98 | loss  3.76 | ppl    13.57\n",
      "| epoch  18 |  1200/ 3005 batches | lr 0.08 | ms/batch 13.96 | loss  3.77 | ppl    13.60\n",
      "| epoch  18 |  1400/ 3005 batches | lr 0.08 | ms/batch 13.95 | loss  3.74 | ppl    13.35\n",
      "| epoch  18 |  1600/ 3005 batches | lr 0.08 | ms/batch 14.00 | loss  3.73 | ppl    13.25\n",
      "| epoch  18 |  1800/ 3005 batches | lr 0.08 | ms/batch 14.05 | loss  3.76 | ppl    13.53\n",
      "| epoch  18 |  2000/ 3005 batches | lr 0.08 | ms/batch 13.59 | loss  3.72 | ppl    13.21\n",
      "| epoch  18 |  2200/ 3005 batches | lr 0.08 | ms/batch 13.43 | loss  3.64 | ppl    12.50\n",
      "| epoch  18 |  2400/ 3005 batches | lr 0.08 | ms/batch 13.40 | loss  3.59 | ppl    12.01\n",
      "| epoch  18 |  2600/ 3005 batches | lr 0.08 | ms/batch 13.37 | loss  3.60 | ppl    12.16\n",
      "| epoch  18 |  2800/ 3005 batches | lr 0.08 | ms/batch 13.86 | loss  3.63 | ppl    12.39\n",
      "| epoch  18 |  3000/ 3005 batches | lr 0.08 | ms/batch 13.91 | loss  3.39 | ppl    10.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 43.12s | valid loss  4.71 | valid ppl    26.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |   200/ 3005 batches | lr 0.08 | ms/batch 14.03 | loss  3.84 | ppl    14.34\n",
      "| epoch  19 |   400/ 3005 batches | lr 0.08 | ms/batch 13.97 | loss  3.78 | ppl    13.78\n",
      "| epoch  19 |   600/ 3005 batches | lr 0.08 | ms/batch 13.98 | loss  3.72 | ppl    13.15\n",
      "| epoch  19 |   800/ 3005 batches | lr 0.08 | ms/batch 13.89 | loss  3.79 | ppl    13.79\n",
      "| epoch  19 |  1000/ 3005 batches | lr 0.08 | ms/batch 13.89 | loss  3.76 | ppl    13.59\n",
      "| epoch  19 |  1200/ 3005 batches | lr 0.08 | ms/batch 13.91 | loss  3.76 | ppl    13.55\n",
      "| epoch  19 |  1400/ 3005 batches | lr 0.08 | ms/batch 13.93 | loss  3.75 | ppl    13.48\n",
      "| epoch  19 |  1600/ 3005 batches | lr 0.08 | ms/batch 14.02 | loss  3.72 | ppl    13.20\n",
      "| epoch  19 |  1800/ 3005 batches | lr 0.08 | ms/batch 13.96 | loss  3.75 | ppl    13.48\n",
      "| epoch  19 |  2000/ 3005 batches | lr 0.08 | ms/batch 13.96 | loss  3.72 | ppl    13.18\n",
      "| epoch  19 |  2200/ 3005 batches | lr 0.08 | ms/batch 13.98 | loss  3.64 | ppl    12.43\n",
      "| epoch  19 |  2400/ 3005 batches | lr 0.08 | ms/batch 13.91 | loss  3.59 | ppl    12.02\n",
      "| epoch  19 |  2600/ 3005 batches | lr 0.08 | ms/batch 14.00 | loss  3.61 | ppl    12.21\n",
      "| epoch  19 |  2800/ 3005 batches | lr 0.08 | ms/batch 13.96 | loss  3.64 | ppl    12.47\n",
      "| epoch  19 |  3000/ 3005 batches | lr 0.08 | ms/batch 13.96 | loss  3.39 | ppl    10.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 43.48s | valid loss  4.71 | valid ppl    26.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |   200/ 3005 batches | lr 0.08 | ms/batch 14.01 | loss  3.83 | ppl    14.25\n",
      "| epoch  20 |   400/ 3005 batches | lr 0.08 | ms/batch 13.94 | loss  3.79 | ppl    13.80\n",
      "| epoch  20 |   600/ 3005 batches | lr 0.08 | ms/batch 13.92 | loss  3.73 | ppl    13.23\n",
      "| epoch  20 |   800/ 3005 batches | lr 0.08 | ms/batch 13.92 | loss  3.78 | ppl    13.77\n",
      "| epoch  20 |  1000/ 3005 batches | lr 0.08 | ms/batch 13.84 | loss  3.76 | ppl    13.57\n",
      "| epoch  20 |  1200/ 3005 batches | lr 0.08 | ms/batch 13.41 | loss  3.76 | ppl    13.53\n",
      "| epoch  20 |  1400/ 3005 batches | lr 0.08 | ms/batch 13.48 | loss  3.75 | ppl    13.41\n",
      "| epoch  20 |  1600/ 3005 batches | lr 0.08 | ms/batch 13.87 | loss  3.72 | ppl    13.19\n",
      "| epoch  20 |  1800/ 3005 batches | lr 0.08 | ms/batch 13.96 | loss  3.76 | ppl    13.52\n",
      "| epoch  20 |  2000/ 3005 batches | lr 0.08 | ms/batch 13.92 | loss  3.72 | ppl    13.17\n",
      "| epoch  20 |  2200/ 3005 batches | lr 0.08 | ms/batch 13.91 | loss  3.64 | ppl    12.45\n",
      "| epoch  20 |  2400/ 3005 batches | lr 0.08 | ms/batch 13.87 | loss  3.58 | ppl    11.97\n",
      "| epoch  20 |  2600/ 3005 batches | lr 0.08 | ms/batch 13.94 | loss  3.61 | ppl    12.20\n",
      "| epoch  20 |  2800/ 3005 batches | lr 0.08 | ms/batch 13.92 | loss  3.63 | ppl    12.39\n",
      "| epoch  20 |  3000/ 3005 batches | lr 0.08 | ms/batch 13.86 | loss  3.39 | ppl    10.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 43.16s | valid loss  4.71 | valid ppl    26.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |   200/ 3005 batches | lr 0.02 | ms/batch 13.95 | loss  3.84 | ppl    14.36\n",
      "| epoch  21 |   400/ 3005 batches | lr 0.02 | ms/batch 13.94 | loss  3.79 | ppl    13.81\n",
      "| epoch  21 |   600/ 3005 batches | lr 0.02 | ms/batch 13.92 | loss  3.73 | ppl    13.24\n",
      "| epoch  21 |   800/ 3005 batches | lr 0.02 | ms/batch 13.97 | loss  3.79 | ppl    13.80\n",
      "| epoch  21 |  1000/ 3005 batches | lr 0.02 | ms/batch 14.11 | loss  3.76 | ppl    13.51\n",
      "| epoch  21 |  1200/ 3005 batches | lr 0.02 | ms/batch 14.04 | loss  3.75 | ppl    13.50\n",
      "| epoch  21 |  1400/ 3005 batches | lr 0.02 | ms/batch 14.03 | loss  3.74 | ppl    13.39\n",
      "| epoch  21 |  1600/ 3005 batches | lr 0.02 | ms/batch 14.01 | loss  3.72 | ppl    13.18\n",
      "| epoch  21 |  1800/ 3005 batches | lr 0.02 | ms/batch 13.95 | loss  3.75 | ppl    13.50\n",
      "| epoch  21 |  2000/ 3005 batches | lr 0.02 | ms/batch 13.99 | loss  3.72 | ppl    13.14\n",
      "| epoch  21 |  2200/ 3005 batches | lr 0.02 | ms/batch 14.00 | loss  3.63 | ppl    12.39\n",
      "| epoch  21 |  2400/ 3005 batches | lr 0.02 | ms/batch 14.08 | loss  3.58 | ppl    11.96\n",
      "| epoch  21 |  2600/ 3005 batches | lr 0.02 | ms/batch 14.12 | loss  3.60 | ppl    12.17\n",
      "| epoch  21 |  2800/ 3005 batches | lr 0.02 | ms/batch 14.00 | loss  3.63 | ppl    12.37\n",
      "| epoch  21 |  3000/ 3005 batches | lr 0.02 | ms/batch 13.96 | loss  3.39 | ppl    10.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 43.63s | valid loss  4.71 | valid ppl    26.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |   200/ 3005 batches | lr 0.02 | ms/batch 14.07 | loss  3.83 | ppl    14.21\n",
      "| epoch  22 |   400/ 3005 batches | lr 0.02 | ms/batch 13.95 | loss  3.78 | ppl    13.77\n",
      "| epoch  22 |   600/ 3005 batches | lr 0.02 | ms/batch 13.98 | loss  3.73 | ppl    13.24\n",
      "| epoch  22 |   800/ 3005 batches | lr 0.02 | ms/batch 13.94 | loss  3.78 | ppl    13.75\n",
      "| epoch  22 |  1000/ 3005 batches | lr 0.02 | ms/batch 13.97 | loss  3.76 | ppl    13.53\n",
      "| epoch  22 |  1200/ 3005 batches | lr 0.02 | ms/batch 13.99 | loss  3.75 | ppl    13.47\n",
      "| epoch  22 |  1400/ 3005 batches | lr 0.02 | ms/batch 14.05 | loss  3.74 | ppl    13.33\n",
      "| epoch  22 |  1600/ 3005 batches | lr 0.02 | ms/batch 13.96 | loss  3.72 | ppl    13.22\n",
      "| epoch  22 |  1800/ 3005 batches | lr 0.02 | ms/batch 13.95 | loss  3.75 | ppl    13.49\n",
      "| epoch  22 |  2000/ 3005 batches | lr 0.02 | ms/batch 13.99 | loss  3.72 | ppl    13.17\n",
      "| epoch  22 |  2200/ 3005 batches | lr 0.02 | ms/batch 14.05 | loss  3.63 | ppl    12.41\n",
      "| epoch  22 |  2400/ 3005 batches | lr 0.02 | ms/batch 13.94 | loss  3.58 | ppl    11.93\n",
      "| epoch  22 |  2600/ 3005 batches | lr 0.02 | ms/batch 13.94 | loss  3.60 | ppl    12.16\n",
      "| epoch  22 |  2800/ 3005 batches | lr 0.02 | ms/batch 13.88 | loss  3.63 | ppl    12.38\n",
      "| epoch  22 |  3000/ 3005 batches | lr 0.02 | ms/batch 13.94 | loss  3.38 | ppl    10.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 43.53s | valid loss  4.71 | valid ppl    26.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |   200/ 3005 batches | lr 0.02 | ms/batch 14.05 | loss  3.83 | ppl    14.18\n",
      "| epoch  23 |   400/ 3005 batches | lr 0.02 | ms/batch 13.98 | loss  3.78 | ppl    13.78\n",
      "| epoch  23 |   600/ 3005 batches | lr 0.02 | ms/batch 13.99 | loss  3.72 | ppl    13.21\n",
      "| epoch  23 |   800/ 3005 batches | lr 0.02 | ms/batch 13.93 | loss  3.78 | ppl    13.74\n",
      "| epoch  23 |  1000/ 3005 batches | lr 0.02 | ms/batch 14.00 | loss  3.76 | ppl    13.51\n",
      "| epoch  23 |  1200/ 3005 batches | lr 0.02 | ms/batch 14.00 | loss  3.75 | ppl    13.48\n",
      "| epoch  23 |  1400/ 3005 batches | lr 0.02 | ms/batch 13.99 | loss  3.74 | ppl    13.37\n",
      "| epoch  23 |  1600/ 3005 batches | lr 0.02 | ms/batch 13.97 | loss  3.72 | ppl    13.20\n",
      "| epoch  23 |  1800/ 3005 batches | lr 0.02 | ms/batch 13.99 | loss  3.76 | ppl    13.52\n",
      "| epoch  23 |  2000/ 3005 batches | lr 0.02 | ms/batch 14.02 | loss  3.72 | ppl    13.19\n",
      "| epoch  23 |  2200/ 3005 batches | lr 0.02 | ms/batch 13.96 | loss  3.63 | ppl    12.41\n",
      "| epoch  23 |  2400/ 3005 batches | lr 0.02 | ms/batch 13.98 | loss  3.59 | ppl    12.00\n",
      "| epoch  23 |  2600/ 3005 batches | lr 0.02 | ms/batch 14.00 | loss  3.62 | ppl    12.26\n",
      "| epoch  23 |  2800/ 3005 batches | lr 0.02 | ms/batch 13.97 | loss  3.62 | ppl    12.32\n",
      "| epoch  23 |  3000/ 3005 batches | lr 0.02 | ms/batch 13.92 | loss  3.38 | ppl    10.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 43.56s | valid loss  4.71 | valid ppl    26.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |   200/ 3005 batches | lr 0.02 | ms/batch 14.04 | loss  3.83 | ppl    14.27\n",
      "| epoch  24 |   400/ 3005 batches | lr 0.02 | ms/batch 14.01 | loss  3.78 | ppl    13.74\n",
      "| epoch  24 |   600/ 3005 batches | lr 0.02 | ms/batch 13.95 | loss  3.72 | ppl    13.19\n",
      "| epoch  24 |   800/ 3005 batches | lr 0.02 | ms/batch 13.98 | loss  3.78 | ppl    13.71\n",
      "| epoch  24 |  1000/ 3005 batches | lr 0.02 | ms/batch 14.00 | loss  3.75 | ppl    13.48\n",
      "| epoch  24 |  1200/ 3005 batches | lr 0.02 | ms/batch 13.94 | loss  3.75 | ppl    13.49\n",
      "| epoch  24 |  1400/ 3005 batches | lr 0.02 | ms/batch 13.88 | loss  3.73 | ppl    13.30\n",
      "| epoch  24 |  1600/ 3005 batches | lr 0.02 | ms/batch 13.95 | loss  3.72 | ppl    13.22\n",
      "| epoch  24 |  1800/ 3005 batches | lr 0.02 | ms/batch 13.96 | loss  3.76 | ppl    13.54\n",
      "| epoch  24 |  2000/ 3005 batches | lr 0.02 | ms/batch 13.92 | loss  3.71 | ppl    13.13\n",
      "| epoch  24 |  2200/ 3005 batches | lr 0.02 | ms/batch 14.02 | loss  3.64 | ppl    12.48\n",
      "| epoch  24 |  2400/ 3005 batches | lr 0.02 | ms/batch 13.99 | loss  3.58 | ppl    11.98\n",
      "| epoch  24 |  2600/ 3005 batches | lr 0.02 | ms/batch 13.93 | loss  3.61 | ppl    12.21\n",
      "| epoch  24 |  2800/ 3005 batches | lr 0.02 | ms/batch 13.90 | loss  3.62 | ppl    12.28\n",
      "| epoch  24 |  3000/ 3005 batches | lr 0.02 | ms/batch 13.95 | loss  3.39 | ppl    10.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 43.50s | valid loss  4.71 | valid ppl    26.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 |   200/ 3005 batches | lr 0.02 | ms/batch 14.00 | loss  3.83 | ppl    14.23\n",
      "| epoch  25 |   400/ 3005 batches | lr 0.02 | ms/batch 13.94 | loss  3.77 | ppl    13.68\n",
      "| epoch  25 |   600/ 3005 batches | lr 0.02 | ms/batch 13.90 | loss  3.72 | ppl    13.16\n",
      "| epoch  25 |   800/ 3005 batches | lr 0.02 | ms/batch 13.88 | loss  3.78 | ppl    13.71\n",
      "| epoch  25 |  1000/ 3005 batches | lr 0.02 | ms/batch 13.83 | loss  3.75 | ppl    13.47\n",
      "| epoch  25 |  1200/ 3005 batches | lr 0.02 | ms/batch 13.84 | loss  3.75 | ppl    13.44\n",
      "| epoch  25 |  1400/ 3005 batches | lr 0.02 | ms/batch 13.94 | loss  3.74 | ppl    13.39\n",
      "| epoch  25 |  1600/ 3005 batches | lr 0.02 | ms/batch 13.95 | loss  3.72 | ppl    13.21\n",
      "| epoch  25 |  1800/ 3005 batches | lr 0.02 | ms/batch 13.85 | loss  3.76 | ppl    13.50\n",
      "| epoch  25 |  2000/ 3005 batches | lr 0.02 | ms/batch 13.84 | loss  3.72 | ppl    13.22\n",
      "| epoch  25 |  2200/ 3005 batches | lr 0.02 | ms/batch 13.80 | loss  3.64 | ppl    12.49\n",
      "| epoch  25 |  2400/ 3005 batches | lr 0.02 | ms/batch 13.71 | loss  3.58 | ppl    11.96\n",
      "| epoch  25 |  2600/ 3005 batches | lr 0.02 | ms/batch 13.39 | loss  3.61 | ppl    12.23\n",
      "| epoch  25 |  2800/ 3005 batches | lr 0.02 | ms/batch 13.38 | loss  3.63 | ppl    12.35\n",
      "| epoch  25 |  3000/ 3005 batches | lr 0.02 | ms/batch 13.37 | loss  3.38 | ppl    10.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 43.03s | valid loss  4.71 | valid ppl    26.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |   200/ 3005 batches | lr 0.00 | ms/batch 13.91 | loss  3.84 | ppl    14.29\n",
      "| epoch  26 |   400/ 3005 batches | lr 0.00 | ms/batch 13.85 | loss  3.78 | ppl    13.72\n",
      "| epoch  26 |   600/ 3005 batches | lr 0.00 | ms/batch 13.85 | loss  3.72 | ppl    13.17\n",
      "| epoch  26 |   800/ 3005 batches | lr 0.00 | ms/batch 13.71 | loss  3.78 | ppl    13.76\n",
      "| epoch  26 |  1000/ 3005 batches | lr 0.00 | ms/batch 13.72 | loss  3.76 | ppl    13.54\n",
      "| epoch  26 |  1200/ 3005 batches | lr 0.00 | ms/batch 13.72 | loss  3.75 | ppl    13.50\n",
      "| epoch  26 |  1400/ 3005 batches | lr 0.00 | ms/batch 13.77 | loss  3.74 | ppl    13.34\n",
      "| epoch  26 |  1600/ 3005 batches | lr 0.00 | ms/batch 13.75 | loss  3.72 | ppl    13.13\n",
      "| epoch  26 |  1800/ 3005 batches | lr 0.00 | ms/batch 13.76 | loss  3.75 | ppl    13.46\n",
      "| epoch  26 |  2000/ 3005 batches | lr 0.00 | ms/batch 13.76 | loss  3.72 | ppl    13.14\n",
      "| epoch  26 |  2200/ 3005 batches | lr 0.00 | ms/batch 13.83 | loss  3.63 | ppl    12.42\n",
      "| epoch  26 |  2400/ 3005 batches | lr 0.00 | ms/batch 13.79 | loss  3.58 | ppl    11.99\n",
      "| epoch  26 |  2600/ 3005 batches | lr 0.00 | ms/batch 13.87 | loss  3.60 | ppl    12.11\n",
      "| epoch  26 |  2800/ 3005 batches | lr 0.00 | ms/batch 13.86 | loss  3.63 | ppl    12.37\n",
      "| epoch  26 |  3000/ 3005 batches | lr 0.00 | ms/batch 13.72 | loss  3.38 | ppl    10.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 42.96s | valid loss  4.71 | valid ppl    26.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |   200/ 3005 batches | lr 0.00 | ms/batch 13.87 | loss  3.84 | ppl    14.28\n",
      "| epoch  27 |   400/ 3005 batches | lr 0.00 | ms/batch 13.83 | loss  3.78 | ppl    13.74\n",
      "| epoch  27 |   600/ 3005 batches | lr 0.00 | ms/batch 13.80 | loss  3.72 | ppl    13.19\n",
      "| epoch  27 |   800/ 3005 batches | lr 0.00 | ms/batch 13.76 | loss  3.78 | ppl    13.76\n",
      "| epoch  27 |  1000/ 3005 batches | lr 0.00 | ms/batch 13.74 | loss  3.75 | ppl    13.48\n",
      "| epoch  27 |  1200/ 3005 batches | lr 0.00 | ms/batch 13.66 | loss  3.74 | ppl    13.40\n",
      "| epoch  27 |  1400/ 3005 batches | lr 0.00 | ms/batch 13.43 | loss  3.74 | ppl    13.32\n",
      "| epoch  27 |  1600/ 3005 batches | lr 0.00 | ms/batch 13.53 | loss  3.72 | ppl    13.16\n",
      "| epoch  27 |  1800/ 3005 batches | lr 0.00 | ms/batch 13.82 | loss  3.76 | ppl    13.51\n",
      "| epoch  27 |  2000/ 3005 batches | lr 0.00 | ms/batch 13.74 | loss  3.72 | ppl    13.20\n",
      "| epoch  27 |  2200/ 3005 batches | lr 0.00 | ms/batch 13.82 | loss  3.63 | ppl    12.38\n",
      "| epoch  27 |  2400/ 3005 batches | lr 0.00 | ms/batch 13.79 | loss  3.58 | ppl    11.98\n",
      "| epoch  27 |  2600/ 3005 batches | lr 0.00 | ms/batch 13.80 | loss  3.60 | ppl    12.16\n",
      "| epoch  27 |  2800/ 3005 batches | lr 0.00 | ms/batch 13.79 | loss  3.63 | ppl    12.37\n",
      "| epoch  27 |  3000/ 3005 batches | lr 0.00 | ms/batch 13.84 | loss  3.39 | ppl    10.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 42.83s | valid loss  4.71 | valid ppl    26.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |   200/ 3005 batches | lr 0.00 | ms/batch 13.86 | loss  3.83 | ppl    14.27\n",
      "| epoch  28 |   400/ 3005 batches | lr 0.00 | ms/batch 13.79 | loss  3.78 | ppl    13.74\n",
      "| epoch  28 |   600/ 3005 batches | lr 0.00 | ms/batch 13.76 | loss  3.73 | ppl    13.25\n",
      "| epoch  28 |   800/ 3005 batches | lr 0.00 | ms/batch 13.79 | loss  3.78 | ppl    13.74\n",
      "| epoch  28 |  1000/ 3005 batches | lr 0.00 | ms/batch 13.68 | loss  3.75 | ppl    13.47\n",
      "| epoch  28 |  1200/ 3005 batches | lr 0.00 | ms/batch 13.75 | loss  3.76 | ppl    13.51\n",
      "| epoch  28 |  1400/ 3005 batches | lr 0.00 | ms/batch 13.77 | loss  3.74 | ppl    13.38\n",
      "| epoch  28 |  1600/ 3005 batches | lr 0.00 | ms/batch 13.86 | loss  3.72 | ppl    13.20\n",
      "| epoch  28 |  1800/ 3005 batches | lr 0.00 | ms/batch 13.80 | loss  3.75 | ppl    13.47\n",
      "| epoch  28 |  2000/ 3005 batches | lr 0.00 | ms/batch 13.74 | loss  3.72 | ppl    13.20\n",
      "| epoch  28 |  2200/ 3005 batches | lr 0.00 | ms/batch 13.68 | loss  3.64 | ppl    12.43\n",
      "| epoch  28 |  2400/ 3005 batches | lr 0.00 | ms/batch 13.74 | loss  3.58 | ppl    11.96\n",
      "| epoch  28 |  2600/ 3005 batches | lr 0.00 | ms/batch 13.87 | loss  3.60 | ppl    12.14\n",
      "| epoch  28 |  2800/ 3005 batches | lr 0.00 | ms/batch 13.89 | loss  3.62 | ppl    12.29\n",
      "| epoch  28 |  3000/ 3005 batches | lr 0.00 | ms/batch 13.84 | loss  3.38 | ppl    10.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 43.00s | valid loss  4.71 | valid ppl    26.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |   200/ 3005 batches | lr 0.00 | ms/batch 13.82 | loss  3.83 | ppl    14.22\n",
      "| epoch  29 |   400/ 3005 batches | lr 0.00 | ms/batch 13.75 | loss  3.78 | ppl    13.77\n",
      "| epoch  29 |   600/ 3005 batches | lr 0.00 | ms/batch 13.77 | loss  3.72 | ppl    13.18\n",
      "| epoch  29 |   800/ 3005 batches | lr 0.00 | ms/batch 13.78 | loss  3.78 | ppl    13.71\n",
      "| epoch  29 |  1000/ 3005 batches | lr 0.00 | ms/batch 13.78 | loss  3.76 | ppl    13.50\n",
      "| epoch  29 |  1200/ 3005 batches | lr 0.00 | ms/batch 13.85 | loss  3.75 | ppl    13.45\n",
      "| epoch  29 |  1400/ 3005 batches | lr 0.00 | ms/batch 13.76 | loss  3.74 | ppl    13.35\n",
      "| epoch  29 |  1600/ 3005 batches | lr 0.00 | ms/batch 13.80 | loss  3.72 | ppl    13.15\n",
      "| epoch  29 |  1800/ 3005 batches | lr 0.00 | ms/batch 13.87 | loss  3.76 | ppl    13.53\n",
      "| epoch  29 |  2000/ 3005 batches | lr 0.00 | ms/batch 13.94 | loss  3.72 | ppl    13.17\n",
      "| epoch  29 |  2200/ 3005 batches | lr 0.00 | ms/batch 13.93 | loss  3.64 | ppl    12.45\n",
      "| epoch  29 |  2400/ 3005 batches | lr 0.00 | ms/batch 13.87 | loss  3.58 | ppl    11.95\n",
      "| epoch  29 |  2600/ 3005 batches | lr 0.00 | ms/batch 13.82 | loss  3.60 | ppl    12.13\n",
      "| epoch  29 |  2800/ 3005 batches | lr 0.00 | ms/batch 13.82 | loss  3.63 | ppl    12.34\n",
      "| epoch  29 |  3000/ 3005 batches | lr 0.00 | ms/batch 13.88 | loss  3.39 | ppl    10.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 43.09s | valid loss  4.71 | valid ppl    26.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 |   200/ 3005 batches | lr 0.00 | ms/batch 13.92 | loss  3.83 | ppl    14.23\n",
      "| epoch  30 |   400/ 3005 batches | lr 0.00 | ms/batch 13.89 | loss  3.78 | ppl    13.73\n",
      "| epoch  30 |   600/ 3005 batches | lr 0.00 | ms/batch 13.86 | loss  3.72 | ppl    13.22\n",
      "| epoch  30 |   800/ 3005 batches | lr 0.00 | ms/batch 13.82 | loss  3.78 | ppl    13.70\n",
      "| epoch  30 |  1000/ 3005 batches | lr 0.00 | ms/batch 13.80 | loss  3.75 | ppl    13.43\n",
      "| epoch  30 |  1200/ 3005 batches | lr 0.00 | ms/batch 13.83 | loss  3.75 | ppl    13.47\n",
      "| epoch  30 |  1400/ 3005 batches | lr 0.00 | ms/batch 13.87 | loss  3.74 | ppl    13.34\n",
      "| epoch  30 |  1600/ 3005 batches | lr 0.00 | ms/batch 13.82 | loss  3.72 | ppl    13.16\n",
      "| epoch  30 |  1800/ 3005 batches | lr 0.00 | ms/batch 13.81 | loss  3.75 | ppl    13.49\n",
      "| epoch  30 |  2000/ 3005 batches | lr 0.00 | ms/batch 13.78 | loss  3.72 | ppl    13.14\n",
      "| epoch  30 |  2200/ 3005 batches | lr 0.00 | ms/batch 13.87 | loss  3.63 | ppl    12.42\n",
      "| epoch  30 |  2400/ 3005 batches | lr 0.00 | ms/batch 13.82 | loss  3.58 | ppl    11.98\n",
      "| epoch  30 |  2600/ 3005 batches | lr 0.00 | ms/batch 13.91 | loss  3.60 | ppl    12.16\n",
      "| epoch  30 |  2800/ 3005 batches | lr 0.00 | ms/batch 13.85 | loss  3.63 | ppl    12.41\n",
      "| epoch  30 |  3000/ 3005 batches | lr 0.00 | ms/batch 13.90 | loss  3.39 | ppl    10.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 43.17s | valid loss  4.71 | valid ppl    26.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  31 |   200/ 3005 batches | lr 0.00 | ms/batch 13.99 | loss  3.83 | ppl    14.24\n",
      "| epoch  31 |   400/ 3005 batches | lr 0.00 | ms/batch 13.87 | loss  3.78 | ppl    13.73\n",
      "| epoch  31 |   600/ 3005 batches | lr 0.00 | ms/batch 13.78 | loss  3.72 | ppl    13.17\n",
      "| epoch  31 |   800/ 3005 batches | lr 0.00 | ms/batch 13.78 | loss  3.78 | ppl    13.72\n",
      "| epoch  31 |  1000/ 3005 batches | lr 0.00 | ms/batch 13.79 | loss  3.75 | ppl    13.47\n",
      "| epoch  31 |  1200/ 3005 batches | lr 0.00 | ms/batch 13.86 | loss  3.75 | ppl    13.45\n",
      "| epoch  31 |  1400/ 3005 batches | lr 0.00 | ms/batch 13.79 | loss  3.74 | ppl    13.35\n",
      "| epoch  31 |  1600/ 3005 batches | lr 0.00 | ms/batch 13.80 | loss  3.72 | ppl    13.19\n",
      "| epoch  31 |  1800/ 3005 batches | lr 0.00 | ms/batch 13.71 | loss  3.75 | ppl    13.50\n",
      "| epoch  31 |  2000/ 3005 batches | lr 0.00 | ms/batch 13.83 | loss  3.73 | ppl    13.23\n",
      "| epoch  31 |  2200/ 3005 batches | lr 0.00 | ms/batch 13.82 | loss  3.63 | ppl    12.34\n",
      "| epoch  31 |  2400/ 3005 batches | lr 0.00 | ms/batch 13.80 | loss  3.59 | ppl    12.00\n",
      "| epoch  31 |  2600/ 3005 batches | lr 0.00 | ms/batch 13.79 | loss  3.61 | ppl    12.18\n",
      "| epoch  31 |  2800/ 3005 batches | lr 0.00 | ms/batch 13.81 | loss  3.63 | ppl    12.38\n",
      "| epoch  31 |  3000/ 3005 batches | lr 0.00 | ms/batch 13.76 | loss  3.38 | ppl    10.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 43.05s | valid loss  4.71 | valid ppl    26.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  32 |   200/ 3005 batches | lr 0.00 | ms/batch 13.82 | loss  3.83 | ppl    14.23\n",
      "| epoch  32 |   400/ 3005 batches | lr 0.00 | ms/batch 13.80 | loss  3.79 | ppl    13.79\n",
      "| epoch  32 |   600/ 3005 batches | lr 0.00 | ms/batch 13.77 | loss  3.72 | ppl    13.16\n",
      "| epoch  32 |   800/ 3005 batches | lr 0.00 | ms/batch 13.87 | loss  3.78 | ppl    13.70\n",
      "| epoch  32 |  1000/ 3005 batches | lr 0.00 | ms/batch 13.79 | loss  3.76 | ppl    13.51\n",
      "| epoch  32 |  1200/ 3005 batches | lr 0.00 | ms/batch 13.90 | loss  3.75 | ppl    13.47\n",
      "| epoch  32 |  1400/ 3005 batches | lr 0.00 | ms/batch 13.86 | loss  3.73 | ppl    13.29\n",
      "| epoch  32 |  1600/ 3005 batches | lr 0.00 | ms/batch 13.80 | loss  3.71 | ppl    13.13\n",
      "| epoch  32 |  1800/ 3005 batches | lr 0.00 | ms/batch 13.80 | loss  3.75 | ppl    13.47\n",
      "| epoch  32 |  2000/ 3005 batches | lr 0.00 | ms/batch 13.83 | loss  3.72 | ppl    13.20\n",
      "| epoch  32 |  2200/ 3005 batches | lr 0.00 | ms/batch 13.84 | loss  3.63 | ppl    12.38\n",
      "| epoch  32 |  2400/ 3005 batches | lr 0.00 | ms/batch 13.82 | loss  3.58 | ppl    11.97\n",
      "| epoch  32 |  2600/ 3005 batches | lr 0.00 | ms/batch 13.78 | loss  3.60 | ppl    12.16\n",
      "| epoch  32 |  2800/ 3005 batches | lr 0.00 | ms/batch 13.78 | loss  3.63 | ppl    12.37\n",
      "| epoch  32 |  3000/ 3005 batches | lr 0.00 | ms/batch 13.76 | loss  3.38 | ppl    10.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 43.15s | valid loss  4.71 | valid ppl    26.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  33 |   200/ 3005 batches | lr 0.00 | ms/batch 13.50 | loss  3.82 | ppl    14.16\n",
      "| epoch  33 |   400/ 3005 batches | lr 0.00 | ms/batch 13.40 | loss  3.78 | ppl    13.72\n",
      "| epoch  33 |   600/ 3005 batches | lr 0.00 | ms/batch 13.38 | loss  3.72 | ppl    13.19\n",
      "| epoch  33 |   800/ 3005 batches | lr 0.00 | ms/batch 13.74 | loss  3.78 | ppl    13.74\n",
      "| epoch  33 |  1000/ 3005 batches | lr 0.00 | ms/batch 13.80 | loss  3.75 | ppl    13.45\n",
      "| epoch  33 |  1200/ 3005 batches | lr 0.00 | ms/batch 13.76 | loss  3.76 | ppl    13.51\n",
      "| epoch  33 |  1400/ 3005 batches | lr 0.00 | ms/batch 13.71 | loss  3.74 | ppl    13.35\n",
      "| epoch  33 |  1600/ 3005 batches | lr 0.00 | ms/batch 13.75 | loss  3.72 | ppl    13.15\n",
      "| epoch  33 |  1800/ 3005 batches | lr 0.00 | ms/batch 13.75 | loss  3.75 | ppl    13.46\n",
      "| epoch  33 |  2000/ 3005 batches | lr 0.00 | ms/batch 13.78 | loss  3.71 | ppl    13.12\n",
      "| epoch  33 |  2200/ 3005 batches | lr 0.00 | ms/batch 13.77 | loss  3.63 | ppl    12.41\n",
      "| epoch  33 |  2400/ 3005 batches | lr 0.00 | ms/batch 13.82 | loss  3.58 | ppl    11.97\n",
      "| epoch  33 |  2600/ 3005 batches | lr 0.00 | ms/batch 13.77 | loss  3.61 | ppl    12.18\n",
      "| epoch  33 |  2800/ 3005 batches | lr 0.00 | ms/batch 13.83 | loss  3.63 | ppl    12.34\n",
      "| epoch  33 |  3000/ 3005 batches | lr 0.00 | ms/batch 13.74 | loss  3.39 | ppl    10.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 42.69s | valid loss  4.71 | valid ppl    26.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  34 |   200/ 3005 batches | lr 0.00 | ms/batch 13.87 | loss  3.82 | ppl    14.13\n",
      "| epoch  34 |   400/ 3005 batches | lr 0.00 | ms/batch 13.85 | loss  3.78 | ppl    13.77\n",
      "| epoch  34 |   600/ 3005 batches | lr 0.00 | ms/batch 13.82 | loss  3.72 | ppl    13.20\n",
      "| epoch  34 |   800/ 3005 batches | lr 0.00 | ms/batch 13.80 | loss  3.78 | ppl    13.69\n",
      "| epoch  34 |  1000/ 3005 batches | lr 0.00 | ms/batch 13.80 | loss  3.76 | ppl    13.51\n",
      "| epoch  34 |  1200/ 3005 batches | lr 0.00 | ms/batch 13.73 | loss  3.76 | ppl    13.52\n",
      "| epoch  34 |  1400/ 3005 batches | lr 0.00 | ms/batch 13.81 | loss  3.73 | ppl    13.31\n",
      "| epoch  34 |  1600/ 3005 batches | lr 0.00 | ms/batch 13.50 | loss  3.72 | ppl    13.17\n",
      "| epoch  34 |  1800/ 3005 batches | lr 0.00 | ms/batch 13.41 | loss  3.76 | ppl    13.52\n",
      "| epoch  34 |  2000/ 3005 batches | lr 0.00 | ms/batch 13.72 | loss  3.71 | ppl    13.10\n",
      "| epoch  34 |  2200/ 3005 batches | lr 0.00 | ms/batch 13.84 | loss  3.64 | ppl    12.43\n",
      "| epoch  34 |  2400/ 3005 batches | lr 0.00 | ms/batch 13.88 | loss  3.58 | ppl    11.97\n",
      "| epoch  34 |  2600/ 3005 batches | lr 0.00 | ms/batch 13.85 | loss  3.60 | ppl    12.13\n",
      "| epoch  34 |  2800/ 3005 batches | lr 0.00 | ms/batch 13.83 | loss  3.62 | ppl    12.32\n",
      "| epoch  34 |  3000/ 3005 batches | lr 0.00 | ms/batch 13.80 | loss  3.39 | ppl    10.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 42.91s | valid loss  4.71 | valid ppl    26.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  35 |   200/ 3005 batches | lr 0.00 | ms/batch 13.86 | loss  3.83 | ppl    14.23\n",
      "| epoch  35 |   400/ 3005 batches | lr 0.00 | ms/batch 13.80 | loss  3.78 | ppl    13.74\n",
      "| epoch  35 |   600/ 3005 batches | lr 0.00 | ms/batch 13.91 | loss  3.72 | ppl    13.15\n",
      "| epoch  35 |   800/ 3005 batches | lr 0.00 | ms/batch 13.95 | loss  3.77 | ppl    13.67\n",
      "| epoch  35 |  1000/ 3005 batches | lr 0.00 | ms/batch 13.80 | loss  3.75 | ppl    13.45\n",
      "| epoch  35 |  1200/ 3005 batches | lr 0.00 | ms/batch 13.82 | loss  3.75 | ppl    13.46\n",
      "| epoch  35 |  1400/ 3005 batches | lr 0.00 | ms/batch 13.80 | loss  3.73 | ppl    13.25\n",
      "| epoch  35 |  1600/ 3005 batches | lr 0.00 | ms/batch 13.78 | loss  3.72 | ppl    13.15\n",
      "| epoch  35 |  1800/ 3005 batches | lr 0.00 | ms/batch 13.81 | loss  3.76 | ppl    13.52\n",
      "| epoch  35 |  2000/ 3005 batches | lr 0.00 | ms/batch 13.79 | loss  3.72 | ppl    13.15\n",
      "| epoch  35 |  2200/ 3005 batches | lr 0.00 | ms/batch 13.77 | loss  3.63 | ppl    12.39\n",
      "| epoch  35 |  2400/ 3005 batches | lr 0.00 | ms/batch 13.78 | loss  3.58 | ppl    11.99\n",
      "| epoch  35 |  2600/ 3005 batches | lr 0.00 | ms/batch 14.04 | loss  3.61 | ppl    12.20\n",
      "| epoch  35 |  2800/ 3005 batches | lr 0.00 | ms/batch 13.83 | loss  3.62 | ppl    12.31\n",
      "| epoch  35 |  3000/ 3005 batches | lr 0.00 | ms/batch 13.92 | loss  3.39 | ppl    10.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 43.15s | valid loss  4.71 | valid ppl    26.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  36 |   200/ 3005 batches | lr 0.00 | ms/batch 13.88 | loss  3.83 | ppl    14.25\n",
      "| epoch  36 |   400/ 3005 batches | lr 0.00 | ms/batch 13.84 | loss  3.78 | ppl    13.69\n",
      "| epoch  36 |   600/ 3005 batches | lr 0.00 | ms/batch 13.88 | loss  3.71 | ppl    13.12\n",
      "| epoch  36 |   800/ 3005 batches | lr 0.00 | ms/batch 13.79 | loss  3.78 | ppl    13.76\n",
      "| epoch  36 |  1000/ 3005 batches | lr 0.00 | ms/batch 13.89 | loss  3.76 | ppl    13.54\n",
      "| epoch  36 |  1200/ 3005 batches | lr 0.00 | ms/batch 13.79 | loss  3.75 | ppl    13.50\n",
      "| epoch  36 |  1400/ 3005 batches | lr 0.00 | ms/batch 13.76 | loss  3.74 | ppl    13.36\n",
      "| epoch  36 |  1600/ 3005 batches | lr 0.00 | ms/batch 13.78 | loss  3.72 | ppl    13.15\n",
      "| epoch  36 |  1800/ 3005 batches | lr 0.00 | ms/batch 13.81 | loss  3.75 | ppl    13.47\n",
      "| epoch  36 |  2000/ 3005 batches | lr 0.00 | ms/batch 13.87 | loss  3.72 | ppl    13.17\n",
      "| epoch  36 |  2200/ 3005 batches | lr 0.00 | ms/batch 13.78 | loss  3.63 | ppl    12.39\n",
      "| epoch  36 |  2400/ 3005 batches | lr 0.00 | ms/batch 13.85 | loss  3.58 | ppl    11.96\n",
      "| epoch  36 |  2600/ 3005 batches | lr 0.00 | ms/batch 13.85 | loss  3.60 | ppl    12.17\n",
      "| epoch  36 |  2800/ 3005 batches | lr 0.00 | ms/batch 13.90 | loss  3.62 | ppl    12.33\n",
      "| epoch  36 |  3000/ 3005 batches | lr 0.00 | ms/batch 13.83 | loss  3.38 | ppl    10.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 43.09s | valid loss  4.71 | valid ppl    26.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  37 |   200/ 3005 batches | lr 0.00 | ms/batch 13.86 | loss  3.83 | ppl    14.17\n",
      "| epoch  37 |   400/ 3005 batches | lr 0.00 | ms/batch 13.91 | loss  3.77 | ppl    13.65\n",
      "| epoch  37 |   600/ 3005 batches | lr 0.00 | ms/batch 13.90 | loss  3.71 | ppl    13.11\n",
      "| epoch  37 |   800/ 3005 batches | lr 0.00 | ms/batch 13.90 | loss  3.77 | ppl    13.67\n",
      "| epoch  37 |  1000/ 3005 batches | lr 0.00 | ms/batch 13.88 | loss  3.74 | ppl    13.40\n",
      "| epoch  37 |  1200/ 3005 batches | lr 0.00 | ms/batch 13.91 | loss  3.75 | ppl    13.45\n",
      "| epoch  37 |  1400/ 3005 batches | lr 0.00 | ms/batch 13.80 | loss  3.74 | ppl    13.34\n",
      "| epoch  37 |  1600/ 3005 batches | lr 0.00 | ms/batch 13.86 | loss  3.73 | ppl    13.27\n",
      "| epoch  37 |  1800/ 3005 batches | lr 0.00 | ms/batch 13.82 | loss  3.76 | ppl    13.55\n",
      "| epoch  37 |  2000/ 3005 batches | lr 0.00 | ms/batch 13.85 | loss  3.72 | ppl    13.14\n",
      "| epoch  37 |  2200/ 3005 batches | lr 0.00 | ms/batch 13.80 | loss  3.63 | ppl    12.40\n",
      "| epoch  37 |  2400/ 3005 batches | lr 0.00 | ms/batch 13.87 | loss  3.58 | ppl    11.93\n",
      "| epoch  37 |  2600/ 3005 batches | lr 0.00 | ms/batch 13.89 | loss  3.61 | ppl    12.21\n",
      "| epoch  37 |  2800/ 3005 batches | lr 0.00 | ms/batch 13.84 | loss  3.63 | ppl    12.40\n",
      "| epoch  37 |  3000/ 3005 batches | lr 0.00 | ms/batch 13.86 | loss  3.38 | ppl    10.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 43.22s | valid loss  4.71 | valid ppl    26.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  38 |   200/ 3005 batches | lr 0.00 | ms/batch 13.93 | loss  3.83 | ppl    14.22\n",
      "| epoch  38 |   400/ 3005 batches | lr 0.00 | ms/batch 13.86 | loss  3.78 | ppl    13.75\n",
      "| epoch  38 |   600/ 3005 batches | lr 0.00 | ms/batch 13.93 | loss  3.72 | ppl    13.18\n",
      "| epoch  38 |   800/ 3005 batches | lr 0.00 | ms/batch 13.91 | loss  3.78 | ppl    13.71\n",
      "| epoch  38 |  1000/ 3005 batches | lr 0.00 | ms/batch 13.93 | loss  3.76 | ppl    13.58\n",
      "| epoch  38 |  1200/ 3005 batches | lr 0.00 | ms/batch 13.88 | loss  3.75 | ppl    13.43\n",
      "| epoch  38 |  1400/ 3005 batches | lr 0.00 | ms/batch 13.84 | loss  3.74 | ppl    13.37\n",
      "| epoch  38 |  1600/ 3005 batches | lr 0.00 | ms/batch 13.86 | loss  3.72 | ppl    13.14\n",
      "| epoch  38 |  1800/ 3005 batches | lr 0.00 | ms/batch 13.87 | loss  3.76 | ppl    13.51\n",
      "| epoch  38 |  2000/ 3005 batches | lr 0.00 | ms/batch 13.83 | loss  3.71 | ppl    13.11\n",
      "| epoch  38 |  2200/ 3005 batches | lr 0.00 | ms/batch 13.80 | loss  3.63 | ppl    12.37\n",
      "| epoch  38 |  2400/ 3005 batches | lr 0.00 | ms/batch 13.80 | loss  3.58 | ppl    12.00\n",
      "| epoch  38 |  2600/ 3005 batches | lr 0.00 | ms/batch 13.77 | loss  3.60 | ppl    12.15\n",
      "| epoch  38 |  2800/ 3005 batches | lr 0.00 | ms/batch 13.83 | loss  3.62 | ppl    12.32\n",
      "| epoch  38 |  3000/ 3005 batches | lr 0.00 | ms/batch 13.77 | loss  3.38 | ppl    10.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 43.16s | valid loss  4.71 | valid ppl    26.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  39 |   200/ 3005 batches | lr 0.00 | ms/batch 13.82 | loss  3.83 | ppl    14.17\n",
      "| epoch  39 |   400/ 3005 batches | lr 0.00 | ms/batch 13.74 | loss  3.78 | ppl    13.78\n",
      "| epoch  39 |   600/ 3005 batches | lr 0.00 | ms/batch 13.78 | loss  3.72 | ppl    13.14\n",
      "| epoch  39 |   800/ 3005 batches | lr 0.00 | ms/batch 13.73 | loss  3.78 | ppl    13.73\n",
      "| epoch  39 |  1000/ 3005 batches | lr 0.00 | ms/batch 13.75 | loss  3.75 | ppl    13.47\n",
      "| epoch  39 |  1200/ 3005 batches | lr 0.00 | ms/batch 13.78 | loss  3.75 | ppl    13.42\n",
      "| epoch  39 |  1400/ 3005 batches | lr 0.00 | ms/batch 13.82 | loss  3.73 | ppl    13.27\n",
      "| epoch  39 |  1600/ 3005 batches | lr 0.00 | ms/batch 13.78 | loss  3.72 | ppl    13.17\n",
      "| epoch  39 |  1800/ 3005 batches | lr 0.00 | ms/batch 13.77 | loss  3.75 | ppl    13.47\n",
      "| epoch  39 |  2000/ 3005 batches | lr 0.00 | ms/batch 13.71 | loss  3.72 | ppl    13.14\n",
      "| epoch  39 |  2200/ 3005 batches | lr 0.00 | ms/batch 13.72 | loss  3.63 | ppl    12.36\n",
      "| epoch  39 |  2400/ 3005 batches | lr 0.00 | ms/batch 13.77 | loss  3.58 | ppl    11.99\n",
      "| epoch  39 |  2600/ 3005 batches | lr 0.00 | ms/batch 13.68 | loss  3.61 | ppl    12.17\n",
      "| epoch  39 |  2800/ 3005 batches | lr 0.00 | ms/batch 13.74 | loss  3.63 | ppl    12.34\n",
      "| epoch  39 |  3000/ 3005 batches | lr 0.00 | ms/batch 13.72 | loss  3.38 | ppl    10.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 42.85s | valid loss  4.71 | valid ppl    26.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  40 |   200/ 3005 batches | lr 0.00 | ms/batch 13.89 | loss  3.83 | ppl    14.20\n",
      "| epoch  40 |   400/ 3005 batches | lr 0.00 | ms/batch 13.76 | loss  3.77 | ppl    13.66\n",
      "| epoch  40 |   600/ 3005 batches | lr 0.00 | ms/batch 13.66 | loss  3.72 | ppl    13.15\n",
      "| epoch  40 |   800/ 3005 batches | lr 0.00 | ms/batch 13.38 | loss  3.77 | ppl    13.64\n",
      "| epoch  40 |  1000/ 3005 batches | lr 0.00 | ms/batch 13.41 | loss  3.76 | ppl    13.51\n",
      "| epoch  40 |  1200/ 3005 batches | lr 0.00 | ms/batch 13.40 | loss  3.75 | ppl    13.43\n",
      "| epoch  40 |  1400/ 3005 batches | lr 0.00 | ms/batch 13.63 | loss  3.74 | ppl    13.34\n",
      "| epoch  40 |  1600/ 3005 batches | lr 0.00 | ms/batch 13.84 | loss  3.72 | ppl    13.15\n",
      "| epoch  40 |  1800/ 3005 batches | lr 0.00 | ms/batch 13.82 | loss  3.75 | ppl    13.49\n",
      "| epoch  40 |  2000/ 3005 batches | lr 0.00 | ms/batch 13.82 | loss  3.72 | ppl    13.14\n",
      "| epoch  40 |  2200/ 3005 batches | lr 0.00 | ms/batch 13.82 | loss  3.63 | ppl    12.40\n",
      "| epoch  40 |  2400/ 3005 batches | lr 0.00 | ms/batch 13.84 | loss  3.58 | ppl    12.00\n",
      "| epoch  40 |  2600/ 3005 batches | lr 0.00 | ms/batch 13.88 | loss  3.60 | ppl    12.13\n",
      "| epoch  40 |  2800/ 3005 batches | lr 0.00 | ms/batch 13.86 | loss  3.63 | ppl    12.40\n",
      "| epoch  40 |  3000/ 3005 batches | lr 0.00 | ms/batch 13.89 | loss  3.38 | ppl    10.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 42.80s | valid loss  4.71 | valid ppl    26.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  4.82 | test ppl    28.28\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "!rm -rf logs\n",
    "!mkdir logs\n",
    "writer = SummaryWriter('./logs')\n",
    "\n",
    "# Loop over epochs.\n",
    "lr = args['lr']\n",
    "best_val_loss = None\n",
    "\n",
    "# Initialize the optimizer\n",
    "#learning_rate = args['lr']\n",
    "#optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.00005)\n",
    "\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "\n",
    "    for epoch in range(1, args['epochs']+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(epoch)\n",
    "        \n",
    "        val_loss = evaluate(val_data)\n",
    "        writer.add_scalar('Eval/Loss', val_loss, epoch)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, 2**(val_loss)))\n",
    "        print('-' * 89)\n",
    "        logging.debug('-' * 89)\n",
    "        logging.debug('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, 2**(val_loss)))\n",
    "        logging.debug('-' * 89)\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(args['save'], 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "            lr /= 4.0\n",
    "        #if lr < 0.5:\n",
    "            #lr=0.5\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')\n",
    "    logging.debug('-' * 89)\n",
    "    logging.debug('Exiting from training early')\n",
    "    \n",
    "writer.close()\n",
    "\n",
    "# Load the best saved model.\n",
    "with open(args['save'], 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "    # after load the rnn params are not a continuous chunk of memory\n",
    "    # this makes them a continuous chunk, and will speed up forward pass\n",
    "    model.rnn.flatten_parameters()\n",
    "\n",
    "# Run on test data.\n",
    "test_loss = evaluate(test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, 2**(test_loss)))\n",
    "print('=' * 89)\n",
    "logging.debug('=' * 89)\n",
    "logging.debug('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, 2**(test_loss)))\n",
    "logging.debug('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "8c4405049469ea0d402664da4ca96ae3dc5594ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Generated 0/200 words\n",
      "| Generated 30/200 words\n",
      "| Generated 60/200 words\n",
      "| Generated 90/200 words\n",
      "| Generated 120/200 words\n",
      "| Generated 150/200 words\n",
      "| Generated 180/200 words\n"
     ]
    }
   ],
   "source": [
    "generate_args={\n",
    "    \"temperature\": 1, #temperature - higher will increase diversity\n",
    "    \"words\":200, #number of words to generate\n",
    "    \"outf\":\"generated.txt\",\n",
    "    \"log_interval\":30,\n",
    "}\n",
    "\n",
    "with open(args['save'], 'rb') as f:\n",
    "    model = torch.load(f).to(device)\n",
    "model.eval()\n",
    "seed_word = \"happy\"\n",
    "seed=torch.LongTensor(1,1).to(device)\n",
    "seed[0]=corpus.dictionary.word2idx[seed_word]\n",
    "hidden = model.init_hidden(1)\n",
    "#input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
    "input = seed\n",
    "with open(generate_args['outf'], 'w') as outf:\n",
    "    outf.write(seed_word + ' ')\n",
    "    with torch.no_grad():  # no tracking history\n",
    "        for i in range(generate_args['words']):\n",
    "            output, hidden = model(input, hidden)\n",
    "            word_weights = output.squeeze().div(generate_args['temperature']).exp().cpu()\n",
    "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "            input.fill_(word_idx)\n",
    "            word = corpus.dictionary.idx2word[word_idx]\n",
    "            \n",
    "            outf.write(word + ' ')\n",
    "\n",
    "            if i % generate_args['log_interval'] == 0:\n",
    "                print('| Generated {}/{} words'.format(i, generate_args['words']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "b99eb4ab2c3b405e22cd8d9fe369cb3a23d482a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy company come on open my bet goodbye my little foot go , the best thing that somebody put , right through you keep on sailin ' ya treat me my love in and i am the bonds of the woman , and through the clouds nights will make it coming on tight if you whiter your faded mind , hey i 'll be free to take you to the room when you go wasting your time and color a letter and if you thrill me to you i 'll love you forever leaving ya , baby and dream again jump and dance and reeling , julia , mmm darling little lady , mm ... oh yeah yeah , yeah , yeah , yeah , yeah , yeah , oh , oh ... i wish i 'd be in your hand again . ooh , everything ever stopped will be you closing in your heart yes , yes you are , aah oh ah , girl , have you been strong speeching la , la , la , la , la la la la laa la la laa la da tschada shuop i love you my pda end_song bridge ball is "
     ]
    }
   ],
   "source": [
    "!cat generated.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip \\n!unzip -o ngrok-stable-linux-amd64.zip\\n\\nLOG_DIR = \\'./logs\\' # Here you have to put your log directory\\nget_ipython().system_raw(\\n    \\'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &\\'\\n    .format(LOG_DIR)\\n)\\nget_ipython().system_raw(\\'./ngrok http 6006 &\\')\\n! curl -s http://localhost:4040/api/tunnels | python3 -c     \"import sys, json; print(json.load(sys.stdin)[\\'tunnels\\'][0][\\'public_url\\'])\"\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### For posting logs to tensorboard  ##\n",
    "# At first in settings, Make sure that Internet option is set to \"Internet Connected\"\n",
    "# After executing this cell, there will come a link below, open that to view your tensor-board\n",
    "'''\n",
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip \n",
    "!unzip -o ngrok-stable-linux-amd64.zip\n",
    "\n",
    "LOG_DIR = './logs' # Here you have to put your log directory\n",
    "get_ipython().system_raw(\n",
    "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
    "    .format(LOG_DIR)\n",
    ")\n",
    "get_ipython().system_raw('./ngrok http 6006 &')\n",
    "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
